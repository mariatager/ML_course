{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary questions (exercise 1)\n",
    "### a. What does each column of tx represent?\n",
    "The first coloumn of tx corresponds to the offset term. The second column corresponds to the heights of the people.\n",
    "### b. What does each row of tx represent?\n",
    "Each row corresponds to the parameters of one datapoint.\n",
    "### c. Why do we have 1’s in tx ?\n",
    "Corresponds to the offset term.\n",
    "### d. If we have heights and weights of 3 people, what would be the size of y and X ̃ ? What would tx[3,2] represent?\n",
    "3 people would mean matrix Y has a size of (3,1), tx has a size of (3,2). tx[3,2] would be the height of the third person.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # compute loss by MSE\n",
    "    # e = y - np.dot(tx,w)\n",
    "    # MSE = (1/(2*len(y)))*np.dot(e.T, e)\n",
    "    # return MSE\n",
    "    \n",
    "    # compute loss by MAE\n",
    "    e = y - np.dot(tx,w)\n",
    "    MAE = (1/len(y))*np.sum(np.absolute(e))\n",
    "    return MAE\n",
    "    \n",
    "    # ***************************************************\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.29392200210518\n"
     ]
    }
   ],
   "source": [
    "w = np.array([1,2])\n",
    "print(compute_loss(y, tx, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "        \n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    # compute loss for each combination of w0 and w1.\n",
    "    for i in range(0,len(grid_w0)):\n",
    "        for j in range(0,len(grid_w1)):\n",
    "            w = np.array([grid_w0[i],grid_w1[j]])\n",
    "            losses[i,j] = compute_loss(y, tx, w) \n",
    "    # ***************************************************\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=4.8736571178918675, w0*=71.42857142857142, w1*=15.306122448979579, execution time=0.369 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=50)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # compute gradient vector\n",
    "    e = y - np.dot(tx,w)\n",
    "    GD = -(1/len(y))*np.dot(tx.T, e)\n",
    "    \n",
    "    return GD\n",
    "    # ***************************************************\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26.706078    6.52028757]\n",
      "[-23.293922    -3.47971243]\n"
     ]
    }
   ],
   "source": [
    "w1 = np.array([100,20])\n",
    "print(compute_gradient(y, tx, w1))\n",
    "\n",
    "w2 = np.array([50,10])\n",
    "print(compute_gradient(y, tx, w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # Compute gradient and loss\n",
    "        e = y - np.dot(tx,w)\n",
    "        \n",
    "        GD = -(1/len(y))*np.dot(tx.T, e)\n",
    "        \n",
    "        loss = (1/(2*len(y)))*np.dot(e.T, e)\n",
    "        # ***************************************************\n",
    "       \n",
    "        # Update w by gradient\n",
    "        w = w - gamma * GD\n",
    "        # ***************************************************\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2792.236712759168, w0=7.329392200210515, w1=1.3479712434988953\n",
      "GD iter. 1/49: loss=2264.635056030003, w0=13.925845180399989, w1=2.5611453626479084\n",
      "GD iter. 2/49: loss=1837.2777140793808, w0=19.86265286257051, w1=3.6530020698820165\n",
      "GD iter. 3/49: loss=1491.1182670993753, w0=25.20577977652398, w1=4.635673106392713\n",
      "GD iter. 4/49: loss=1210.7291150455721, w0=30.014593999082102, w1=5.520077039252339\n",
      "GD iter. 5/49: loss=983.6139018819906, w0=34.34252679938441, w1=6.316040578826006\n",
      "GD iter. 6/49: loss=799.6505792194902, w0=38.23766631965648, w1=7.032407764442307\n",
      "GD iter. 7/49: loss=650.6402878628649, w0=41.74329188790136, w1=7.677138231496977\n",
      "GD iter. 8/49: loss=529.9419518639978, w0=44.898354899321745, w1=8.257395651846178\n",
      "GD iter. 9/49: loss=432.1762997049158, w0=47.73791160960009, w1=8.779627330160462\n",
      "GD iter. 10/49: loss=352.9861214560593, w0=50.293512648850594, w1=9.249635840643318\n",
      "GD iter. 11/49: loss=288.84207707448576, w0=52.59355358417606, w1=9.672643500077887\n",
      "GD iter. 12/49: loss=236.88540112541105, w0=54.66359042596897, w1=10.053350393569001\n",
      "GD iter. 13/49: loss=194.80049360666052, w0=56.52662358358259, w1=10.395986597711005\n",
      "GD iter. 14/49: loss=160.7117185164726, w0=58.203353425434855, w1=10.704359181438807\n",
      "GD iter. 15/49: loss=133.09981069342035, w0=59.71241028310189, w1=10.98189450679383\n",
      "GD iter. 16/49: loss=110.7341653567481, w0=61.07056145500222, w1=11.231676299613351\n",
      "GD iter. 17/49: loss=92.61799263404352, w0=62.29289750971252, w1=11.456479913150918\n",
      "GD iter. 18/49: loss=77.94389272865283, w0=63.39299995895178, w1=11.65880316533473\n",
      "GD iter. 19/49: loss=66.0578718052864, w0=64.38309216326712, w1=11.84089409230016\n",
      "GD iter. 20/49: loss=56.43019485735961, w0=65.27417514715093, w1=12.00477592656905\n",
      "GD iter. 21/49: loss=48.63177652953887, w0=66.07614983264635, w1=12.15226957741105\n",
      "GD iter. 22/49: loss=42.3150576840041, w0=66.79792704959223, w1=12.285013863168848\n",
      "GD iter. 23/49: loss=37.1985154191209, w0=67.44752654484353, w1=12.404483720350868\n",
      "GD iter. 24/49: loss=33.05411618456553, w0=68.03216609056969, w1=12.512006591814686\n",
      "GD iter. 25/49: loss=29.697152804575687, w0=68.55834168172323, w1=12.608777176132122\n",
      "GD iter. 26/49: loss=26.978012466783923, w0=69.03189971376143, w1=12.695870702017814\n",
      "GD iter. 27/49: loss=24.775508793172552, w0=69.4581019425958, w1=12.774254875314938\n",
      "GD iter. 28/49: loss=22.991480817547366, w0=69.84168394854674, w1=12.844800631282348\n",
      "GD iter. 29/49: loss=21.54641815729094, w0=70.18690775390259, w1=12.908291811653019\n",
      "GD iter. 30/49: loss=20.375917402483257, w0=70.49760917872285, w1=12.96543387398662\n",
      "GD iter. 31/49: loss=19.427811791089002, w0=70.77724046106108, w1=13.016861730086863\n",
      "GD iter. 32/49: loss=18.659846245859693, w0=71.02890861516549, w1=13.06314680057708\n",
      "GD iter. 33/49: loss=18.037794154223942, w0=71.25540995385946, w1=13.104803364018276\n",
      "GD iter. 34/49: loss=17.53393195999899, w0=71.45926115868403, w1=13.142294271115352\n",
      "GD iter. 35/49: loss=17.12580358267677, w0=71.64272724302614, w1=13.176036087502721\n",
      "GD iter. 36/49: loss=16.79521959704577, w0=71.80784671893404, w1=13.206403722251354\n",
      "GD iter. 37/49: loss=16.52744656868467, w0=71.95645424725116, w1=13.233734593525122\n",
      "GD iter. 38/49: loss=16.310550415712168, w0=72.09020102273657, w1=13.258332377671515\n",
      "GD iter. 39/49: loss=16.134864531804432, w0=72.21057312067343, w1=13.280470383403268\n",
      "GD iter. 40/49: loss=15.992558965839178, w0=72.3189080088166, w1=13.300394588561845\n",
      "GD iter. 41/49: loss=15.877291457407322, w0=72.41640940814547, w1=13.318326373204565\n",
      "GD iter. 42/49: loss=15.783924775577509, w0=72.50416066754144, w1=13.334464979383013\n",
      "GD iter. 43/49: loss=15.708297763295365, w0=72.58313680099782, w1=13.348989724943616\n",
      "GD iter. 44/49: loss=15.647039883346828, w0=72.65421532110855, w1=13.362061995948158\n",
      "GD iter. 45/49: loss=15.597421000588524, w0=72.71818598920821, w1=13.373827039852248\n",
      "GD iter. 46/49: loss=15.557229705554292, w0=72.77575959049791, w1=13.384415579365928\n",
      "GD iter. 47/49: loss=15.524674756576564, w0=72.82757583165863, w1=13.39394526492824\n",
      "GD iter. 48/49: loss=15.498305247904602, w0=72.8742104487033, w1=13.40252198193432\n",
      "GD iter. 49/49: loss=15.476945945880319, w0=72.91618160404349, w1=13.410241027239794\n",
      "GD: execution time=0.016 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813313d90bb74228a57d754f7fa47977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ***************************************************\n",
    "    # Implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    e = y - np.dot(tx,w)\n",
    "    GD = -(1/len(y))*np.dot(tx.T, e)\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # Implement stochastic gradient descent.\n",
    "        e = y - np.dot(tx,w)\n",
    "        GD = -(1/len(y))*np.dot(tx.T, e)\n",
    "        loss = (1/(2*len(y)))*np.dot(e.T, e)\n",
    "  \n",
    "        # Update w by gradient\n",
    "        w = w - gamma * GD\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        # ***************************************************\n",
    "        print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2792.236712759168, w0=7.329392200210515, w1=1.3479712434988953\n",
      "SGD iter. 1/49: loss=2264.635056030003, w0=13.925845180399989, w1=2.5611453626479084\n",
      "SGD iter. 2/49: loss=1837.2777140793808, w0=19.86265286257051, w1=3.6530020698820165\n",
      "SGD iter. 3/49: loss=1491.1182670993753, w0=25.20577977652398, w1=4.635673106392713\n",
      "SGD iter. 4/49: loss=1210.7291150455721, w0=30.014593999082102, w1=5.520077039252339\n",
      "SGD iter. 5/49: loss=983.6139018819906, w0=34.34252679938441, w1=6.316040578826006\n",
      "SGD iter. 6/49: loss=799.6505792194902, w0=38.23766631965648, w1=7.032407764442307\n",
      "SGD iter. 7/49: loss=650.6402878628649, w0=41.74329188790136, w1=7.677138231496977\n",
      "SGD iter. 8/49: loss=529.9419518639978, w0=44.898354899321745, w1=8.257395651846178\n",
      "SGD iter. 9/49: loss=432.1762997049158, w0=47.73791160960009, w1=8.779627330160462\n",
      "SGD iter. 10/49: loss=352.9861214560593, w0=50.293512648850594, w1=9.249635840643318\n",
      "SGD iter. 11/49: loss=288.84207707448576, w0=52.59355358417606, w1=9.672643500077887\n",
      "SGD iter. 12/49: loss=236.88540112541105, w0=54.66359042596897, w1=10.053350393569001\n",
      "SGD iter. 13/49: loss=194.80049360666052, w0=56.52662358358259, w1=10.395986597711005\n",
      "SGD iter. 14/49: loss=160.7117185164726, w0=58.203353425434855, w1=10.704359181438807\n",
      "SGD iter. 15/49: loss=133.09981069342035, w0=59.71241028310189, w1=10.98189450679383\n",
      "SGD iter. 16/49: loss=110.7341653567481, w0=61.07056145500222, w1=11.231676299613351\n",
      "SGD iter. 17/49: loss=92.61799263404352, w0=62.29289750971252, w1=11.456479913150918\n",
      "SGD iter. 18/49: loss=77.94389272865283, w0=63.39299995895178, w1=11.65880316533473\n",
      "SGD iter. 19/49: loss=66.0578718052864, w0=64.38309216326712, w1=11.84089409230016\n",
      "SGD iter. 20/49: loss=56.43019485735961, w0=65.27417514715093, w1=12.00477592656905\n",
      "SGD iter. 21/49: loss=48.63177652953887, w0=66.07614983264635, w1=12.15226957741105\n",
      "SGD iter. 22/49: loss=42.3150576840041, w0=66.79792704959223, w1=12.285013863168848\n",
      "SGD iter. 23/49: loss=37.1985154191209, w0=67.44752654484353, w1=12.404483720350868\n",
      "SGD iter. 24/49: loss=33.05411618456553, w0=68.03216609056969, w1=12.512006591814686\n",
      "SGD iter. 25/49: loss=29.697152804575687, w0=68.55834168172323, w1=12.608777176132122\n",
      "SGD iter. 26/49: loss=26.978012466783923, w0=69.03189971376143, w1=12.695870702017814\n",
      "SGD iter. 27/49: loss=24.775508793172552, w0=69.4581019425958, w1=12.774254875314938\n",
      "SGD iter. 28/49: loss=22.991480817547366, w0=69.84168394854674, w1=12.844800631282348\n",
      "SGD iter. 29/49: loss=21.54641815729094, w0=70.18690775390259, w1=12.908291811653019\n",
      "SGD iter. 30/49: loss=20.375917402483257, w0=70.49760917872285, w1=12.96543387398662\n",
      "SGD iter. 31/49: loss=19.427811791089002, w0=70.77724046106108, w1=13.016861730086863\n",
      "SGD iter. 32/49: loss=18.659846245859693, w0=71.02890861516549, w1=13.06314680057708\n",
      "SGD iter. 33/49: loss=18.037794154223942, w0=71.25540995385946, w1=13.104803364018276\n",
      "SGD iter. 34/49: loss=17.53393195999899, w0=71.45926115868403, w1=13.142294271115352\n",
      "SGD iter. 35/49: loss=17.12580358267677, w0=71.64272724302614, w1=13.176036087502721\n",
      "SGD iter. 36/49: loss=16.79521959704577, w0=71.80784671893404, w1=13.206403722251354\n",
      "SGD iter. 37/49: loss=16.52744656868467, w0=71.95645424725116, w1=13.233734593525122\n",
      "SGD iter. 38/49: loss=16.310550415712168, w0=72.09020102273657, w1=13.258332377671515\n",
      "SGD iter. 39/49: loss=16.134864531804432, w0=72.21057312067343, w1=13.280470383403268\n",
      "SGD iter. 40/49: loss=15.992558965839178, w0=72.3189080088166, w1=13.300394588561845\n",
      "SGD iter. 41/49: loss=15.877291457407322, w0=72.41640940814547, w1=13.318326373204565\n",
      "SGD iter. 42/49: loss=15.783924775577509, w0=72.50416066754144, w1=13.334464979383013\n",
      "SGD iter. 43/49: loss=15.708297763295365, w0=72.58313680099782, w1=13.348989724943616\n",
      "SGD iter. 44/49: loss=15.647039883346828, w0=72.65421532110855, w1=13.362061995948158\n",
      "SGD iter. 45/49: loss=15.597421000588524, w0=72.71818598920821, w1=13.373827039852248\n",
      "SGD iter. 46/49: loss=15.557229705554292, w0=72.77575959049791, w1=13.384415579365928\n",
      "SGD iter. 47/49: loss=15.524674756576564, w0=72.82757583165863, w1=13.39394526492824\n",
      "SGD iter. 48/49: loss=15.498305247904602, w0=72.8742104487033, w1=13.40252198193432\n",
      "SGD iter. 49/49: loss=15.476945945880319, w0=72.91618160404349, w1=13.410241027239794\n",
      "SGD: execution time=0.014 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859fc76a3962405fb75260676b493ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# Reload the data by subsampling first, then by subsampling and adding outliers\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)\n",
    "# ***************************************************\n",
    "\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.8351145358533, w0=51.847464098448484, w1=7.724426406192441\n",
      "GD iter. 1/49: loss=318.28212470159497, w0=67.401703327983, w1=10.041754328050121\n",
      "GD iter. 2/49: loss=88.6423556165126, w0=72.06797509684336, w1=10.736952704607413\n",
      "GD iter. 3/49: loss=67.9747763988552, w0=73.46785662750146, w1=10.945512217574594\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631796\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.05160722578589, w1=11.03248153448191\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536943\n",
      "GD iter. 8/49: loss=65.93074217249237, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889338, w0=74.06736849193958, w1=11.034829706038407\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003893\n",
      "GD iter. 11/49: loss=65.93073011140233, w0=74.06776649225756, w1=11.034889001593537\n",
      "GD iter. 12/49: loss=65.93073010339529, w0=74.06779404612573, w1=11.034893106670431\n",
      "GD iter. 13/49: loss=65.93073010267466, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260396, w0=74.06780553608876, w1=11.034894818487498\n",
      "GD iter. 16/49: loss=65.93073010260343, w0=74.06780575927509, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260338, w0=74.06780582623098, w1=11.034894861713957\n",
      "GD iter. 18/49: loss=65.93073010260339, w0=74.06780584631775, w1=11.034894864706558\n",
      "GD iter. 19/49: loss=65.93073010260338, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260338, w0=74.06780585415159, w1=11.034894865873671\n",
      "GD iter. 21/49: loss=65.93073010260339, w0=74.06780585469393, w1=11.03489486595447\n",
      "GD iter. 22/49: loss=65.93073010260338, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260338, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.0678058549201, w1=11.034894865988164\n",
      "GD iter. 25/49: loss=65.93073010260338, w0=74.06780585492449, w1=11.034894865988818\n",
      "GD iter. 26/49: loss=65.93073010260338, w0=74.06780585492581, w1=11.034894865989017\n",
      "GD iter. 27/49: loss=65.93073010260338, w0=74.06780585492619, w1=11.034894865989077\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.06780585492632, w1=11.034894865989095\n",
      "GD iter. 29/49: loss=65.93073010260336, w0=74.06780585492635, w1=11.034894865989099\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.034894865989102\n",
      "GD iter. 31/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.004 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points \n",
    "#       and the model fit\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "# ***************************************************\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef96e37df7a4214adb511d0fdd2789b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # Compute subgradient gradient vector for MAE\n",
    "    e = y - np.dot(tx,w)\n",
    "    sign = np.sign(e)\n",
    "    g = -(1/len(y))*np.dot(sign,tx)\n",
    "     \n",
    "    return g\n",
    "    # ***************************************************\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        \n",
    "        # compute subgradient and loss\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        subgradient = compute_subgradient_mae(y, tx, w)\n",
    "        \n",
    "        # update w by subgradient\n",
    "        w = w - subgradient * loss\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=74.06780585492638, w0=74.06780585492638, w1=9.265323718695322e-14\n",
      "SubGD iter. 1/499: loss=13.561731565085076, w0=73.66498214507236, w1=9.511349913666642\n",
      "SubGD iter. 2/499: loss=7.416634966318797, w0=73.59155011570286, w1=13.501850638547138\n",
      "SubGD iter. 3/499: loss=5.635119664315051, w0=73.36837705969039, w1=15.188374846791081\n",
      "SubGD iter. 4/499: loss=5.368618587503157, w0=72.94313994384855, w1=15.502563495897409\n",
      "SubGD iter. 5/499: loss=5.332294283084711, w0=72.78475496514307, w1=15.793972557729766\n",
      "SubGD iter. 6/499: loss=5.312983046843682, w0=72.52173600242803, w1=15.983707637603938\n",
      "SubGD iter. 7/499: loss=5.313206731511542, w0=72.67955402415609, w1=16.010522061356525\n",
      "SubGD iter. 8/499: loss=5.311184980102729, w0=72.62696803425408, w1=15.99439047671623\n",
      "SubGD iter. 9/499: loss=5.310625986426089, w0=72.62696803425408, w1=15.982178610923757\n",
      "SubGD iter. 10/499: loss=5.3105979050536645, w0=72.62696803425408, w1=15.96996680970483\n",
      "SubGD iter. 11/499: loss=5.310585309479802, w0=72.67954808682319, w1=16.003868826204368\n",
      "SubGD iter. 12/499: loss=5.311164713543209, w0=72.62696229758019, w1=15.987737303119399\n",
      "SubGD iter. 13/499: loss=5.310610687351503, w0=72.62696229758019, w1=15.975525472507382\n",
      "SubGD iter. 14/499: loss=5.3105826060599775, w0=72.62696229758019, w1=15.963313706468725\n",
      "SubGD iter. 15/499: loss=5.31062783873734, w0=72.67954277123106, w1=15.997215994468956\n",
      "SubGD iter. 16/499: loss=5.31114445436483, w0=72.62695718257397, w1=15.981084532916894\n",
      "SubGD iter. 17/499: loss=5.310595389204531, w0=72.62695718257397, w1=15.968872737483201\n",
      "SubGD iter. 18/499: loss=5.310592401322646, w0=72.67953730535935, w1=16.00277479925605\n",
      "SubGD iter. 19/499: loss=5.311161283923955, w0=72.62695155007297, w1=15.986643286587812\n",
      "SubGD iter. 20/499: loss=5.310608171643733, w0=72.62695155007297, w1=15.974431461760703\n",
      "SubGD iter. 21/499: loss=5.310580090365509, w0=72.62695155007297, w1=15.962219701506925\n",
      "SubGD iter. 22/499: loss=5.310634929119391, w0=72.67953209392564, w1=15.99612203477114\n",
      "SubGD iter. 23/499: loss=5.311141025981108, w0=72.62694653921297, w1=15.979990583632057\n",
      "SubGD iter. 24/499: loss=5.3105928736513945, w0=72.62694653921297, w1=15.967778793982916\n",
      "SubGD iter. 25/499: loss=5.310599490281186, w0=72.67952673218605, w1=16.001680901010662\n",
      "SubGD iter. 26/499: loss=5.311157856757892, w0=72.62694101083201, w1=15.985549398751706\n",
      "SubGD iter. 27/499: loss=5.3106056562319015, w0=72.62694101083201, w1=15.973337579708826\n",
      "SubGD iter. 28/499: loss=5.310577574966977, w0=72.62694101083201, w1=15.961125825239245\n",
      "SubGD iter. 29/499: loss=5.3106420166178365, w0=72.67952162485793, w1=15.995028203749037\n",
      "SubGD iter. 30/499: loss=5.31113760004997, w0=72.62693610406535, w1=15.978896763015484\n",
      "SubGD iter. 31/499: loss=5.310590358394135, w0=72.62693610406535, w1=15.966684979150216\n",
      "SubGD iter. 32/499: loss=5.310606576356811, w0=72.67951636719759, w1=16.000587131414452\n",
      "SubGD iter. 33/499: loss=5.311154432043811, w0=72.62693067975161, w1=15.98445563955733\n",
      "SubGD iter. 34/499: loss=5.310603141115882, w0=72.62693067975161, w1=15.972243826297998\n",
      "SubGD iter. 35/499: loss=5.3105750598642585, w0=72.62693067975161, w1=15.960032077611933\n",
      "SubGD iter. 36/499: loss=5.310649101234062, w0=72.67951136392224, w1=15.9939345013489\n",
      "SubGD iter. 37/499: loss=5.3111341765702065, w0=72.62692587702551, w1=15.977803071013433\n",
      "SubGD iter. 38/499: loss=5.310587843432625, w0=72.62692587702551, w1=15.965591292931357\n",
      "SubGD iter. 39/499: loss=5.31061365955091, w0=72.6795062102884, w1=15.999493490413693\n",
      "SubGD iter. 40/499: loss=5.311151009780499, w0=72.6269205567262, w1=15.983362008950962\n",
      "SubGD iter. 41/499: loss=5.310600626295552, w0=72.6269205567262, w1=15.971150201474497\n",
      "SubGD iter. 42/499: loss=5.31057822495066, w0=72.67950053915146, w1=16.005052172747416\n",
      "SubGD iter. 43/499: loss=5.31116783693537, w0=72.62691471898378, w1=15.988920640175813\n",
      "SubGD iter. 44/499: loss=5.310613408453189, w0=72.62691471898378, w1=15.97670880330658\n",
      "SubGD iter. 45/499: loss=5.310585327147274, w0=72.62691471898378, w1=15.964497031010742\n",
      "SubGD iter. 46/499: loss=5.310620755638048, w0=72.67949512250495, w1=15.998399273793481\n",
      "SubGD iter. 47/499: loss=5.311147576552292, w0=72.62690950293512, w1=15.982267802758445\n",
      "SubGD iter. 48/499: loss=5.310598110151654, w0=72.62690950293512, w1=15.970056001067892\n",
      "SubGD iter. 49/499: loss=5.310585319612916, w0=72.67948955560456, w1=16.00395801763212\n",
      "SubGD iter. 50/499: loss=5.311164404926015, w0=72.62690376941717, w1=15.987826495484507\n",
      "SubGD iter. 51/499: loss=5.310610892450714, w0=72.62690376941717, w1=15.975614664400862\n",
      "SubGD iter. 52/499: loss=5.310582811158102, w0=72.62690376941717, w1=15.963402897890578\n",
      "SubGD iter. 53/499: loss=5.310627848838832, w0=72.67948424316805, w1=15.997305185955296\n",
      "SubGD iter. 54/499: loss=5.311144145779058, w0=72.62689865756627, w1=15.981173725340497\n",
      "SubGD iter. 55/499: loss=5.310595594303875, w0=72.62689865756627, w1=15.968961929435174\n",
      "SubGD iter. 56/499: loss=5.3105924113895115, w0=72.67947878045132, w1=16.00286399127229\n",
      "SubGD iter. 57/499: loss=5.31116097537103, w0=72.62689302821992, w1=15.986732479541216\n",
      "SubGD iter. 58/499: loss=5.310608376744296, w0=72.62689302821992, w1=15.974520654242475\n",
      "SubGD iter. 59/499: loss=5.310580295464987, w0=72.62689302821992, w1=15.962308893517067\n",
      "SubGD iter. 60/499: loss=5.310634939154653, w0=72.67947357217196, w1=15.996211226845347\n",
      "SubGD iter. 61/499: loss=5.3111407174595895, w0=72.62688802051395, w1=15.980079776643331\n",
      "SubGD iter. 62/499: loss=5.310593078752091, w0=72.62688802051395, w1=15.967867986522558\n",
      "SubGD iter. 63/499: loss=5.310599500281839, w0=72.67946821358605, w1=16.001770093614144\n",
      "SubGD iter. 64/499: loss=5.311157548269207, w0=72.62688249528635, w1=15.985638592292156\n",
      "SubGD iter. 65/499: loss=5.310605861333814, w0=72.62688249528635, w1=15.973426772777641\n",
      "SubGD iter. 66/499: loss=5.3105777800678045, w0=72.62688249528635, w1=15.961215017836427\n",
      "SubGD iter. 67/499: loss=5.3106420265869, w0=72.67946310941097, w1=15.995117396409858\n",
      "SubGD iter. 68/499: loss=5.311137291592676, w0=72.62687759167244, w1=15.97898595661318\n",
      "SubGD iter. 69/499: loss=5.310590563496178, w0=72.62687759167244, w1=15.966774172276276\n",
      "SubGD iter. 70/499: loss=5.310606586291283, w0=72.67945785490305, w1=16.000676324603933\n",
      "SubGD iter. 71/499: loss=5.311154123619334, w0=72.62687217051078, w1=15.984544833683586\n",
      "SubGD iter. 72/499: loss=5.31060334621914, w0=72.62687217051078, w1=15.972333019952615\n",
      "SubGD iter. 73/499: loss=5.310575264966433, w0=72.62687217051078, w1=15.960121270794914\n",
      "SubGD iter. 74/499: loss=5.310649111136963, w0=72.67945285477946, w1=15.9940236945951\n",
      "SubGD iter. 75/499: loss=5.311133868177109, w0=72.62686737093611, w1=15.97789226519631\n",
      "SubGD iter. 76/499: loss=5.310588048536015, w0=72.62686737093611, w1=15.965680486642595\n",
      "SubGD iter. 77/499: loss=5.310613669419236, w0=72.67944770429669, w1=15.999582684187928\n",
      "SubGD iter. 78/499: loss=5.311150701420204, w0=72.62686205378758, w1=15.983451203661774\n",
      "SubGD iter. 79/499: loss=5.310600831400156, w0=72.62686205378758, w1=15.971239395713669\n",
      "SubGD iter. 80/499: loss=5.31057823478442, w0=72.67944203631019, w1=16.005141367049365\n",
      "SubGD iter. 81/499: loss=5.311167528607869, w0=72.62685621919526, w1=15.98900983541424\n",
      "SubGD iter. 82/499: loss=5.310613613559006, w0=72.62685621919526, w1=15.976797998073364\n",
      "SubGD iter. 83/499: loss=5.310585532252007, w0=72.62685621919526, w1=15.964586225305883\n",
      "SubGD iter. 84/499: loss=5.310620765440261, w0=72.67943662281348, w1=15.998488468151198\n",
      "SubGD iter. 85/499: loss=5.311147268256146, w0=72.62685100629609, w1=15.982356998052545\n",
      "SubGD iter. 86/499: loss=5.310598315257599, w0=72.62685100629609, w1=15.970145195890348\n",
      "SubGD iter. 87/499: loss=5.310585329380581, w0=72.67943105906224, w1=16.00404721251693\n",
      "SubGD iter. 88/499: loss=5.311164096662651, w0=72.62684527592697, w1=15.987915691305602\n",
      "SubGD iter. 89/499: loss=5.310611097557871, w0=72.62684527592697, w1=15.97570385975031\n",
      "SubGD iter. 90/499: loss=5.310583016264175, w0=72.62684527592697, w1=15.963492092768382\n",
      "SubGD iter. 91/499: loss=5.310627858574966, w0=72.67942574977424, w1=15.997394380895255\n",
      "SubGD iter. 92/499: loss=5.311143837547035, w0=72.62684016722427, w1=15.981262921216643\n",
      "SubGD iter. 93/499: loss=5.31059579941116, w0=72.62684016722427, w1=15.969051124839673\n",
      "SubGD iter. 94/499: loss=5.3105924210911155, w0=72.67942029020537, w1=16.002953186738722\n",
      "SubGD iter. 95/499: loss=5.311160667171773, w0=72.62683454102546, w1=15.986821675943737\n",
      "SubGD iter. 96/499: loss=5.31060858185279, w0=72.62683454102546, w1=15.974609850173346\n",
      "SubGD iter. 97/499: loss=5.310580500572397, w0=72.62683454102546, w1=15.96239808897629\n",
      "SubGD iter. 98/499: loss=5.310634948824742, w0=72.67941508507323, w1=15.996300422366302\n",
      "SubGD iter. 99/499: loss=5.311140409291659, w0=72.62682953646639, w1=15.98016897310028\n",
      "SubGD iter. 100/499: loss=5.31059328386071, w0=72.62682953646639, w1=15.967957182507856\n",
      "SubGD iter. 101/499: loss=5.310599509917411, w0=72.67940972963389, w1=16.001859289660956\n",
      "SubGD iter. 102/499: loss=5.311157240134027, w0=72.62682401438504, w1=15.985727789274863\n",
      "SubGD iter. 103/499: loss=5.310606066443641, w0=72.62682401438504, w1=15.973515969288695\n",
      "SubGD iter. 104/499: loss=5.31057798517655, w0=72.62682401438504, w1=15.96130421387583\n",
      "SubGD iter. 105/499: loss=5.310642036190975, w0=72.67940462860474, w1=15.995206592510572\n",
      "SubGD iter. 106/499: loss=5.311136983488807, w0=72.62681911391674, w1=15.979075153649692\n",
      "SubGD iter. 107/499: loss=5.310590768606132, w0=72.62681911391674, w1=15.966863368841135\n",
      "SubGD iter. 108/499: loss=5.31060659586086, w0=72.67939937724209, w1=16.000765521229884\n",
      "SubGD iter. 109/499: loss=5.311153815548201, w0=72.62681369590003, w1=15.984634031245236\n",
      "SubGD iter. 110/499: loss=5.310603551330301, w0=72.62681369590003, w1=15.972422217042608\n",
      "SubGD iter. 111/499: loss=5.310575470076507, w0=72.62681369590003, w1=15.960210467413255\n",
      "SubGD iter. 112/499: loss=5.310649120675061, w0=72.67939438026315, w1=15.99411289127433\n",
      "SubGD iter. 113/499: loss=5.311133560137274, w0=72.62680889946971, w1=15.977981462811146\n",
      "SubGD iter. 114/499: loss=5.310588253647296, w0=72.62680889946971, w1=15.965769683785775\n",
      "SubGD iter. 115/499: loss=5.31061367892285, w0=72.6793892329244, w1=15.999671881391777\n",
      "SubGD iter. 116/499: loss=5.311150393413092, w0=72.62680358546486, w1=15.983540401801129\n",
      "SubGD iter. 117/499: loss=5.310601036512644, w0=72.62680358546486, w1=15.971328593381363\n",
      "SubGD iter. 118/499: loss=5.310578244253562, w0=72.67938356808122, w1=16.00523056477751\n",
      "SubGD iter. 119/499: loss=5.311167220633472, w0=72.62679775401554, w1=15.98909903407779\n",
      "SubGD iter. 120/499: loss=5.3106138186727, w0=72.62679775401554, w1=15.976887196265253\n",
      "SubGD iter. 121/499: loss=5.310585737364615, w0=72.62679775401554, w1=15.964675423026112\n",
      "SubGD iter. 122/499: loss=5.310620774877947, w0=72.6793781577272, w1=15.998577665931675\n",
      "SubGD iter. 123/499: loss=5.311146960313024, w0=72.62679254425875, w1=15.982446196768333\n",
      "SubGD iter. 124/499: loss=5.310598520371412, w0=72.62679254425875, w1=15.970234394134472\n",
      "SubGD iter. 125/499: loss=5.3105853387838104, w0=72.67937259711799, w1=16.004136410821083\n",
      "SubGD iter. 126/499: loss=5.311163788752227, w0=72.62678681703133, w1=15.988004890544968\n",
      "SubGD iter. 127/499: loss=5.310611302672888, w0=72.62678681703133, w1=15.97579305851801\n",
      "SubGD iter. 128/499: loss=5.310583221378107, w0=72.62678681703133, w1=15.963581291064418\n",
      "SubGD iter. 129/499: loss=5.310627867946759, w0=72.6793672909714, w1=15.997483579251119\n",
      "SubGD iter. 130/499: loss=5.311143529667872, w0=72.62678171146973, w1=15.981352120507625\n",
      "SubGD iter. 131/499: loss=5.310596004526295, w0=72.62678171146973, w1=15.969140323658989\n",
      "SubGD iter. 132/499: loss=5.310592430428465, w0=72.67936183454329, w1=16.003042385617643\n",
      "SubGD iter. 133/499: loss=5.311160359325295, w0=72.62677608841135, w1=15.986910875757676\n",
      "SubGD iter. 134/499: loss=5.310608786969127, w0=72.62677608841135, w1=15.974699049515618\n",
      "SubGD iter. 135/499: loss=5.310580705687651, w0=72.62677608841135, w1=15.962487287846896\n",
      "SubGD iter. 136/499: loss=5.310634958130672, w0=72.67935663255126, w1=15.996389621296316\n",
      "SubGD iter. 137/499: loss=5.311140101476426, w0=72.6267710869921, w1=15.980258172965218\n",
      "SubGD iter. 138/499: loss=5.310593488977165, w0=72.6267710869921, w1=15.968046381901127\n",
      "SubGD iter. 139/499: loss=5.310599519188918, w0=72.6793512802514, w1=16.001948489113413\n",
      "SubGD iter. 140/499: loss=5.311156932351464, w0=72.62676556804989, w1=15.985816989662144\n",
      "SubGD iter. 141/499: loss=5.310606271561298, w0=72.62676556804989, w1=15.973605169204305\n",
      "SubGD iter. 142/499: loss=5.31057819029312, w0=72.62676556804989, w1=15.96139341331977\n",
      "SubGD iter. 143/499: loss=5.310642045431079, w0=72.67934618236109, w1=15.995295792013502\n",
      "SubGD iter. 144/499: loss=5.311136675737476, w0=72.62676067072012, w1=15.97916435408735\n",
      "SubGD iter. 145/499: loss=5.310590973723902, w0=72.62676067072012, w1=15.966952568807121\n",
      "SubGD iter. 146/499: loss=5.310606605066555, w0=72.67934093413662, w1=16.000854721254637\n",
      "SubGD iter. 147/499: loss=5.311153507829528, w0=72.62675525584127, w1=15.984723232204617\n",
      "SubGD iter. 148/499: loss=5.3106037564492725, w0=72.62675525584127, w1=15.972511417530315\n",
      "SubGD iter. 149/499: loss=5.310575675194395, w0=72.62675525584127, w1=15.96029966742929\n",
      "SubGD iter. 150/499: loss=5.310649129849369, w0=72.67933594029523, w1=15.994202091348933\n",
      "SubGD iter. 151/499: loss=5.311133252449815, w0=72.6267504625482, w1=15.978070663820283\n",
      "SubGD iter. 152/499: loss=5.310588458766383, w0=72.6267504625482, w1=15.965858884323238\n",
      "SubGD iter. 153/499: loss=5.310613688062769, w0=72.67933079609338, w1=15.999761081987588\n",
      "SubGD iter. 154/499: loss=5.311150085758275, w0=72.62674515167993, w1=15.983629603331377\n",
      "SubGD iter. 155/499: loss=5.31060124163293, w0=72.62674515167993, w1=15.971417794439933\n",
      "SubGD iter. 156/499: loss=5.310578253359098, w0=72.67932513438646, w1=16.005319765894207\n",
      "SubGD iter. 157/499: loss=5.311166913011288, w0=72.62673932336655, w1=15.989188236128825\n",
      "SubGD iter. 158/499: loss=5.310614023794182, w0=72.62673932336655, w1=15.976976397844608\n",
      "SubGD iter. 159/499: loss=5.3105859424850115, w0=72.62673932336655, w1=15.96476462413379\n",
      "SubGD iter. 160/499: loss=5.310620783952122, w0=72.67931972716805, w1=15.998666867097283\n",
      "SubGD iter. 161/499: loss=5.311146652722035, w0=72.62673411674506, w1=15.982535398868182\n",
      "SubGD iter. 162/499: loss=5.310598725493008, w0=72.62673411674506, w1=15.970323595762641\n",
      "SubGD iter. 163/499: loss=5.3105853478236185, w0=72.67931416969381, w1=16.00422561250696\n",
      "SubGD iter. 164/499: loss=5.311163481193857, w0=72.62672839265228, w1=15.988094093164987\n",
      "SubGD iter. 165/499: loss=5.310611507795678, w0=72.62672839265228, w1=15.975882260666346\n",
      "SubGD iter. 166/499: loss=5.310583426499813, w0=72.62672839265228, w1=15.963670492741075\n",
      "SubGD iter. 167/499: loss=5.310627876955221, w0=72.67930886668154, w1=15.997572780985283\n",
      "SubGD iter. 168/499: loss=5.311143222140681, w0=72.6267232902247, w1=15.981441323175837\n",
      "SubGD iter. 169/499: loss=5.310596209649196, w0=72.6267232902247, w1=15.969229525855518\n",
      "SubGD iter. 170/499: loss=5.31059243940258, w0=72.6793034133871, w1=16.00313158787146\n",
      "SubGD iter. 171/499: loss=5.31116005183071, w0=72.62671767029967, w1=15.987000078945444\n",
      "SubGD iter. 172/499: loss=5.310608992093226, w0=72.62671767029967, w1=15.974788252231699\n",
      "SubGD iter. 173/499: loss=5.310580910810661, w0=72.62671767029967, w1=15.962576490091294\n",
      "SubGD iter. 174/499: loss=5.310634967073455, w0=72.67929821452812, w1=15.996478823597803\n",
      "SubGD iter. 175/499: loss=5.311139794013005, w0=72.62671267201314, w1=15.980347376200559\n",
      "SubGD iter. 176/499: loss=5.310593694101372, w0=72.62671267201314, w1=15.96813558466478\n",
      "SubGD iter. 177/499: loss=5.31059952809737, w0=72.67929286536064, w1=16.00203769193394\n",
      "SubGD iter. 178/499: loss=5.311156624920635, w0=72.62670715620301, w1=15.985906193416424\n",
      "SubGD iter. 179/499: loss=5.310606476686698, w0=72.62670715620301, w1=15.973694372486895\n",
      "SubGD iter. 180/499: loss=5.310578395417435, w0=72.62670715620301, w1=15.961482616130676\n",
      "SubGD iter. 181/499: loss=5.31064205430822, w0=72.6792877706021, w1=15.995384994881077\n",
      "SubGD iter. 182/499: loss=5.311136368337796, w0=72.6267022620047, w1=15.979253557888585\n",
      "SubGD iter. 183/499: loss=5.3105911788494105, w0=72.6267022620047, w1=15.967041772136668\n",
      "SubGD iter. 184/499: loss=5.310606613909381, w0=72.67928252550875, w1=16.000943924640634\n",
      "SubGD iter. 185/499: loss=5.3111532004624245, w0=72.62669685025665, w1=15.984812436524177\n",
      "SubGD iter. 186/499: loss=5.310603961575973, w0=72.62669685025665, w1=15.972600621378183\n",
      "SubGD iter. 187/499: loss=5.310575880320009, w0=72.62669685025665, w1=15.960388870805467\n",
      "SubGD iter. 188/499: loss=5.310649138660898, w0=72.67927753479785, w1=15.994291294781362\n",
      "SubGD iter. 189/499: loss=5.3111329451138465, w0=72.62669206009375, w1=15.97815986818618\n",
      "SubGD iter. 190/499: loss=5.3105886638931885, w0=72.62669206009375, w1=15.965948088217443\n",
      "SubGD iter. 191/499: loss=5.31061369684, w0=72.67927239372582, w1=15.999850285937827\n",
      "SubGD iter. 192/499: loss=5.311149778454868, w0=72.62668675235498, w1=15.983718808214983\n",
      "SubGD iter. 193/499: loss=5.310601446760925, w0=72.62668675235498, w1=15.971506998851845\n",
      "SubGD iter. 194/499: loss=5.310578262102038, w0=72.67926673514808, w1=16.005408970361934\n",
      "SubGD iter. 195/499: loss=5.311166605740436, w0=72.62668092717045, w1=15.989277441529822\n",
      "SubGD iter. 196/499: loss=5.310614228923367, w0=72.62668092717045, w1=15.977065602773907\n",
      "SubGD iter. 197/499: loss=5.310586147613114, w0=72.62668092717045, w1=15.964853828591394\n",
      "SubGD iter. 198/499: loss=5.310620792663792, w0=72.67926133105821, w1=15.9987560716105\n",
      "SubGD iter. 199/499: loss=5.311146345482295, w0=72.6266757236772, w1=15.982624604314575\n",
      "SubGD iter. 200/499: loss=5.310598930622296, w0=72.6266757236772, w1=15.970412800737337\n",
      "SubGD iter. 201/499: loss=5.310585356501014, w0=72.67925577671186, w1=16.004314817537054\n",
      "SubGD iter. 202/499: loss=5.3111631739866585, w0=72.62667000271199, w1=15.988183299128156\n",
      "SubGD iter. 203/499: loss=5.310611712926157, w0=72.62667000271199, w1=15.975971466157814\n",
      "SubGD iter. 204/499: loss=5.310583631629208, w0=72.62667000271199, w1=15.963759697760844\n",
      "SubGD iter. 205/499: loss=5.310627885601363, w0=72.67925047682685, w1=15.99766198606025\n",
      "SubGD iter. 206/499: loss=5.311142914964581, w0=72.62666490341137, w1=15.981530529183784\n",
      "SubGD iter. 207/499: loss=5.310596414779777, w0=72.62666490341137, w1=15.969318731391764\n",
      "SubGD iter. 208/499: loss=5.310592448014465, w0=72.67924502665903, w1=16.003220793462688\n",
      "SubGD iter. 209/499: loss=5.311159744687133, w0=72.62665928661262, w1=15.987089285469553\n",
      "SubGD iter. 210/499: loss=5.310609197224994, w0=72.62665928661262, w1=15.974877458284105\n",
      "SubGD iter. 211/499: loss=5.310581115941346, w0=72.62665928661262, w1=15.962665695672\n",
      "SubGD iter. 212/499: loss=5.3106349756541045, w0=72.67923983092602, w1=15.996568029233286\n",
      "SubGD iter. 213/499: loss=5.311139486900514, w0=72.62665429145176, w1=15.98043658276883\n",
      "SubGD iter. 214/499: loss=5.3105938992332415, w0=72.62665429145176, w1=15.968224790761347\n",
      "SubGD iter. 215/499: loss=5.310599536643778, w0=72.67923448488388, w1=16.002126898085063\n",
      "SubGD iter. 216/499: loss=5.311156317840654, w0=72.62664877876665, w1=15.985995400500238\n",
      "SubGD iter. 217/499: loss=5.310606681819752, w0=72.62664877876665, w1=15.973783579099003\n",
      "SubGD iter. 218/499: loss=5.310578600549405, w0=72.62664877876665, w1=15.961571822271079\n",
      "SubGD iter. 219/499: loss=5.310642062823407, w0=72.67922939325005, w1=15.99547420107584\n",
      "SubGD iter. 220/499: loss=5.311136061288886, w0=72.62664388769274, w1=15.979342765015945\n",
      "SubGD iter. 221/499: loss=5.310591383982565, w0=72.62664388769274, w1=15.967130978792321\n",
      "SubGD iter. 222/499: loss=5.3106066223903445, w0=72.67922415128076, w1=16.00103313135043\n",
      "SubGD iter. 223/499: loss=5.311152893446008, w0=72.62663847906843, w1=15.984901644166468\n",
      "SubGD iter. 224/499: loss=5.310604166710311, w0=72.62663847906843, w1=15.972689828548766\n",
      "SubGD iter. 225/499: loss=5.310576085453265, w0=72.62663847906843, w1=15.960478077504344\n",
      "SubGD iter. 226/499: loss=5.310649147110658, w0=72.67921916369329, w1=15.994380501534181\n",
      "SubGD iter. 227/499: loss=5.311132638128488, w0=72.62663369202865, w1=15.9782490758714\n",
      "SubGD iter. 228/499: loss=5.310588869027627, w0=72.62663369202865, w1=15.966037295430953\n",
      "SubGD iter. 229/499: loss=5.310613705255551, w0=72.67921402574405, w1=15.999939493205058\n",
      "SubGD iter. 230/499: loss=5.311149471501988, w0=72.62662838741234, w1=15.983808016414518\n",
      "SubGD iter. 231/499: loss=5.310601651896546, w0=72.62662838741234, w1=15.971596206579667\n",
      "SubGD iter. 232/499: loss=5.31057827048339, w0=72.6792083702884, w1=16.00549817814326\n",
      "SubGD iter. 233/499: loss=5.31116629882003, w0=72.62662256534959, w1=15.989366650243351\n",
      "SubGD iter. 234/499: loss=5.310614434060169, w0=72.62662256534959, w1=15.97715481101572\n",
      "SubGD iter. 235/499: loss=5.310586352748832, w0=72.62662256534959, w1=15.964943036361495\n",
      "SubGD iter. 236/499: loss=5.3106208010139655, w0=72.67920296932003, w1=15.998845279433908\n",
      "SubGD iter. 237/499: loss=5.3111460385929234, w0=72.62661736497752, w1=15.982713813070093\n",
      "SubGD iter. 238/499: loss=5.310599135759197, w0=72.62661736497752, w1=15.97050200902114\n",
      "SubGD iter. 239/499: loss=5.310585364817007, w0=72.67919741809452, w1=16.004404025873942\n",
      "SubGD iter. 240/499: loss=5.311162867129744, w0=72.62661164713283, w1=15.988272508397058\n",
      "SubGD iter. 241/499: loss=5.310611918064238, w0=72.62661164713283, w1=15.976060674954999\n",
      "SubGD iter. 242/499: loss=5.310583836766202, w0=72.62661164713283, w1=15.963848906086314\n",
      "SubGD iter. 243/499: loss=5.310627893886194, w0=72.67919212132972, w1=15.997751194438608\n",
      "SubGD iter. 244/499: loss=5.311142608138686, w0=72.6266065509521, w1=15.98161973849406\n",
      "SubGD iter. 245/499: loss=5.310596619917954, w0=72.6266065509521, w1=15.969407940230322\n",
      "SubGD iter. 246/499: loss=5.310592456265129, w0=72.67918667428147, w1=16.003310002353917\n",
      "SubGD iter. 247/499: loss=5.311159437893685, w0=72.62660093727261, w1=15.9871784952926\n",
      "SubGD iter. 248/499: loss=5.310609402364347, w0=72.62660093727261, w1=15.97496666763543\n",
      "SubGD iter. 249/499: loss=5.310581321079616, w0=72.62660093727261, w1=15.962754904551606\n",
      "SubGD iter. 250/499: loss=5.310634983873624, w0=72.6791814816674, w1=15.996657238165364\n",
      "SubGD iter. 251/499: loss=5.31113918013807, w0=72.62659594523039, w1=15.980525792632633\n",
      "SubGD iter. 252/499: loss=5.310594104372689, w0=72.62659594523039, w1=15.96831400015343\n",
      "SubGD iter. 253/499: loss=5.310599544829149, w0=72.67917613874356, w1=16.002216107529403\n",
      "SubGD iter. 254/499: loss=5.31115601111064, w0=72.62659043566326, w1=15.986084610876205\n",
      "SubGD iter. 255/499: loss=5.310606886960379, w0=72.62659043566326, w1=15.973872789003245\n",
      "SubGD iter. 256/499: loss=5.310578805688946, w0=72.62659043566326, w1=15.9616610317036\n",
      "SubGD iter. 257/499: loss=5.310642070977648, w0=72.6791710502274, w1=15.995563410560417\n",
      "SubGD iter. 258/499: loss=5.311135754589862, w0=72.62658554770671, w1=15.979431975432053\n",
      "SubGD iter. 259/499: loss=5.310591589123285, w0=72.62658554770671, w1=15.967220188736704\n",
      "SubGD iter. 260/499: loss=5.310606630510452, w0=72.67916581137513, w1=16.00112234134665\n",
      "SubGD iter. 261/499: loss=5.311152586779401, w0=72.62658014219909, w1=15.984990855094125\n",
      "SubGD iter. 262/499: loss=5.3106043718522065, w0=72.62658014219909, w1=15.972779039004696\n",
      "SubGD iter. 263/499: loss=5.310576290594074, w0=72.62658014219909, w1=15.96056728748855\n",
      "SubGD iter. 264/499: loss=5.310649155199653, w0=72.67916082690404, w1=15.994469711570025\n",
      "SubGD iter. 265/499: loss=5.311132331492854, w0=72.6265753582754, w1=15.978338286838584\n",
      "SubGD iter. 266/499: loss=5.310589074169613, w0=72.6265753582754, w1=15.96612650592641\n",
      "SubGD iter. 267/499: loss=5.310613713310431, w0=72.67915569207055, w1=16.000028703751937\n",
      "SubGD iter. 268/499: loss=5.311149164898757, w0=72.62657005677453, w1=15.983897227892639\n",
      "SubGD iter. 269/499: loss=5.310601857039706, w0=72.62657005677453, w1=15.971685417586059\n",
      "SubGD iter. 270/499: loss=5.310578278504163, w0=72.67915003973002, w1=16.005587389200855\n",
      "SubGD iter. 271/499: loss=5.311165992249192, w0=72.62656423782656, w1=15.98945586223209\n",
      "SubGD iter. 272/499: loss=5.310614639204505, w0=72.62656423782656, w1=15.977244022532727\n",
      "SubGD iter. 273/499: loss=5.310586557892082, w0=72.62656423782656, w1=15.965032247406771\n",
      "SubGD iter. 274/499: loss=5.310620809003649, w0=72.67914464187609, w1=15.99893449053019\n",
      "SubGD iter. 275/499: loss=5.31114573205304, w0=72.62655904056864, w1=15.982803025097423\n",
      "SubGD iter. 276/499: loss=5.31059934090362, w0=72.62655904056864, w1=15.970591220576736\n",
      "SubGD iter. 277/499: loss=5.310585372772602, w0=72.6791390937644, w1=16.00449323748033\n",
      "SubGD iter. 278/499: loss=5.311162560622241, w0=72.62655332583745, w1=15.988361720934394\n",
      "SubGD iter. 279/499: loss=5.310612123209833, w0=72.62655332583745, w1=15.976149887020599\n",
      "SubGD iter. 280/499: loss=5.310584041910714, w0=72.62655332583745, w1=15.963938117680181\n",
      "SubGD iter. 281/499: loss=5.310627901810714, w0=72.67913380011281, w1=15.997840406083064\n",
      "SubGD iter. 282/499: loss=5.311142301662121, w0=72.62654823276962, w1=15.981708951069374\n",
      "SubGD iter. 283/499: loss=5.310596825063636, w0=72.62654823276962, w1=15.9694971523339\n",
      "SubGD iter. 284/499: loss=5.310592464155578, w0=72.6791283561771, w1=16.003399214507866\n",
      "SubGD iter. 285/499: loss=5.311159131449484, w0=72.62654262220235, w1=15.987267708377308\n",
      "SubGD iter. 286/499: loss=5.3106096075112035, w0=72.62654262220235, w1=15.9750558802484\n",
      "SubGD iter. 287/499: loss=5.310581526225387, w0=72.62654262220235, w1=15.962844116692839\n",
      "SubGD iter. 288/499: loss=5.310634991733017, w0=72.67912316667496, w1=15.99674645035677\n",
      "SubGD iter. 289/499: loss=5.311138873724795, w0=72.62653763327174, w1=15.980615005754704\n",
      "SubGD iter. 290/499: loss=5.31059430951963, w0=72.62653763327174, w1=15.968403212803763\n",
      "SubGD iter. 291/499: loss=5.310599552654483, w0=72.67911782686238, w1=16.00230532022969\n",
      "SubGD iter. 292/499: loss=5.311155704729714, w0=72.62653212681555, w1=15.986173824507059\n",
      "SubGD iter. 293/499: loss=5.31060709210849, w0=72.62653212681555, w1=15.973962002162358\n",
      "SubGD iter. 294/499: loss=5.3105790108359745, w0=72.62653212681555, w1=15.961750244390974\n",
      "SubGD iter. 295/499: loss=5.310642078771943, w0=72.67911274145685, w1=15.995652623297548\n",
      "SubGD iter. 296/499: loss=5.311135448239847, w0=72.62652724196933, w1=15.979521189099657\n",
      "SubGD iter. 297/499: loss=5.310591794271478, w0=72.62652724196933, w1=15.967309401932567\n",
      "SubGD iter. 298/499: loss=5.310606638270707, w0=72.67910750571458, w1=16.001211554592054\n",
      "SubGD iter. 299/499: loss=5.31115228046172, w0=72.6265218395714, w1=15.985080069269902\n",
      "SubGD iter. 300/499: loss=5.310604577001569, w0=72.6265218395714, w1=15.972868252708729\n",
      "SubGD iter. 301/499: loss=5.310576495742352, w0=72.6265218395714, w1=15.960656500720841\n",
      "SubGD iter. 302/499: loss=5.310649162928885, w0=72.67910252435287, w1=15.99455892485166\n",
      "SubGD iter. 303/499: loss=5.311132025206071, w0=72.62651705875678, w1=15.978427501050499\n",
      "SubGD iter. 304/499: loss=5.310589279319059, w0=72.62651705875678, w1=15.96621571966658\n",
      "SubGD iter. 305/499: loss=5.310613721005639, w0=72.67909739262812, w1=16.000117917541235\n",
      "SubGD iter. 306/499: loss=5.311148858644295, w0=72.62651176036432, w1=15.98398644261212\n",
      "SubGD iter. 307/499: loss=5.31060206219032, w0=72.62651176036432, w1=15.971774631833792\n",
      "SubGD iter. 308/499: loss=5.310578286165355, w0=72.67909174339566, w1=16.0056766034975\n",
      "SubGD iter. 309/499: loss=5.311165686027044, w0=72.6265059445241, w1=15.989545077458818\n",
      "SubGD iter. 310/499: loss=5.310614844356285, w0=72.6265059445241, w1=15.977333237287704\n",
      "SubGD iter. 311/499: loss=5.310586763042777, w0=72.6265059445241, w1=15.965121461690002\n",
      "SubGD iter. 312/499: loss=5.310620816633845, w0=72.67908634864918, w1=15.999023704862129\n",
      "SubGD iter. 313/499: loss=5.311145425861766, w0=72.62650075037332, w1=15.982892240359353\n",
      "SubGD iter. 314/499: loss=5.310599546055482, w0=72.62650075037332, w1=15.970680435366917\n",
      "SubGD iter. 315/499: loss=5.310585380368796, w0=72.6790808036443, w1=16.004582452319003\n",
      "SubGD iter. 316/499: loss=5.311162254463267, w0=72.62649503874862, w1=15.98845093670296\n",
      "SubGD iter. 317/499: loss=5.3106123283628595, w0=72.62649503874862, w1=15.976239102317413\n",
      "SubGD iter. 318/499: loss=5.310584247062655, w0=72.62649503874862, w1=15.964027332505244\n",
      "SubGD iter. 319/499: loss=5.310627909375931, w0=72.67907551309887, w1=15.997929620956423\n",
      "SubGD iter. 320/499: loss=5.311141995534007, w0=72.62648994878664, w1=15.981798166872531\n",
      "SubGD iter. 321/499: loss=5.310597030216741, w0=72.62648994878664, w1=15.969586367665304\n",
      "SubGD iter. 322/499: loss=5.31059247168681, w0=72.67907007226869, w1=16.003488429887348\n",
      "SubGD iter. 323/499: loss=5.3111588253536555, w0=72.62648434132458, w1=15.987356924686491\n",
      "SubGD iter. 324/499: loss=5.310609812665473, w0=72.62648434132458, w1=15.975145096085829\n",
      "SubGD iter. 325/499: loss=5.31058173137857, w0=72.62648434132458, w1=15.962933332058515\n",
      "SubGD iter. 326/499: loss=5.310634999233286, w0=72.67906488587145, w1=15.996835665770329\n",
      "SubGD iter. 327/499: loss=5.31113856765981, w0=72.62647935549859, w1=15.980704222097868\n",
      "SubGD iter. 328/499: loss=5.310594514673978, w0=72.62647935549859, w1=15.968492428675171\n",
      "SubGD iter. 329/499: loss=5.310599560120784, w0=72.67905954916316, w1=16.002394536148763\n",
      "SubGD iter. 330/499: loss=5.311155398697, w0=72.62647385214636, w1=15.98626304135564\n",
      "SubGD iter. 331/499: loss=5.310607297263999, w0=72.62647385214636, w1=15.97405121853918\n",
      "SubGD iter. 332/499: loss=5.310579215990397, w0=72.62647385214636, w1=15.96183946029604\n",
      "SubGD iter. 333/499: loss=5.310642086207299, w0=72.67905446686129, w1=15.995741839250082\n",
      "SubGD iter. 334/499: loss=5.311135142237964, w0=72.6264689704035, w1=15.979610405981607\n",
      "SubGD iter. 335/499: loss=5.310591999427066, w0=72.6264689704035, w1=15.967398618342758\n",
      "SubGD iter. 336/499: loss=5.310606645672111, w0=72.67904923422203, w1=16.001300771049493\n",
      "SubGD iter. 337/499: loss=5.311151974492094, w0=72.62646357110825, w1=15.985169286656658\n",
      "SubGD iter. 338/499: loss=5.310604782158317, w0=72.62646357110825, w1=15.972957469623724\n",
      "SubGD iter. 339/499: loss=5.310576700898015, w0=72.62646357110825, w1=15.960745717164077\n",
      "SubGD iter. 340/499: loss=5.310649170299361, w0=72.6790442559627, w1=15.994648141341948\n",
      "SubGD iter. 341/499: loss=5.311131719267261, w0=72.62645879339568, w1=15.978516718470011\n",
      "SubGD iter. 342/499: loss=5.310589484475882, w0=72.62645879339568, w1=15.966304936614332\n",
      "SubGD iter. 343/499: loss=5.3106137283421795, w0=72.67903912733966, w1=16.00020713453582\n",
      "SubGD iter. 344/499: loss=5.311148552737727, w0=72.62645349810464, w1=15.98407566053583\n",
      "SubGD iter. 345/499: loss=5.310602267348302, w0=72.62645349810464, w1=15.97186384928574\n",
      "SubGD iter. 346/499: loss=5.310578293467968, w0=72.67903348120828, w1=16.005765820996064\n",
      "SubGD iter. 347/499: loss=5.311165380152711, w0=72.62644768536518, w1=15.989634295886411\n",
      "SubGD iter. 348/499: loss=5.310615049515426, w0=72.62644768536518, w1=15.97742245524353\n",
      "SubGD iter. 349/499: loss=5.310586968200835, w0=72.62644768536518, w1=15.965210679174064\n",
      "SubGD iter. 350/499: loss=5.310620823905554, w0=72.67902808956227, w1=15.999112922392612\n",
      "SubGD iter. 351/499: loss=5.311145120018227, w0=72.62644249431456, w1=15.98298145881877\n",
      "SubGD iter. 352/499: loss=5.310599751214694, w0=72.62644249431456, w1=15.970769653354568\n",
      "SubGD iter. 353/499: loss=5.310585387606593, w0=72.6790225476572, w1=16.004671670352856\n",
      "SubGD iter. 354/499: loss=5.3111619486519475, w0=72.62643678578935, w1=15.98854015566565\n",
      "SubGD iter. 355/499: loss=5.310612533523231, w0=72.62643678578935, w1=15.976328320808333\n",
      "SubGD iter. 356/499: loss=5.310584452221943, w0=72.62643678578935, w1=15.964116550524398\n",
      "SubGD iter. 357/499: loss=5.31062791658284, w0=72.67901726021097, w1=15.998018839021585\n",
      "SubGD iter. 358/499: loss=5.311141689753469, w0=72.62643169892628, w1=15.981887385866436\n",
      "SubGD iter. 359/499: loss=5.310597235377183, w0=72.62643169892628, w1=15.96967558618744\n",
      "SubGD iter. 360/499: loss=5.310592478859825, w0=72.67901182247935, w1=16.003577648455273\n",
      "SubGD iter. 361/499: loss=5.311158519605321, w0=72.62642609456248, w1=15.98744614418306\n",
      "SubGD iter. 362/499: loss=5.310610017827071, w0=72.62642609456248, w1=15.975234315110626\n",
      "SubGD iter. 363/499: loss=5.3105819365390845, w0=72.62642609456248, w1=15.963022550611543\n",
      "SubGD iter. 364/499: loss=5.310635006375428, w0=72.67900663918006, w1=15.99692488436895\n",
      "SubGD iter. 365/499: loss=5.311138261942243, w0=72.62642111183409, w1=15.98079344162504\n",
      "SubGD iter. 366/499: loss=5.310594719835648, w0=72.62642111183409, w1=15.968581647730572\n",
      "SubGD iter. 367/499: loss=5.310599567229051, w0=72.67900130556903, w1=16.002483755249543\n",
      "SubGD iter. 368/499: loss=5.311155093011625, w0=72.62641561157882, w1=15.986352261384873\n",
      "SubGD iter. 369/499: loss=5.310607502426823, w0=72.62641561157882, w1=15.974140438096638\n",
      "SubGD iter. 370/499: loss=5.310579421152137, w0=72.62641561157882, w1=15.961928679381726\n",
      "SubGD iter. 371/499: loss=5.310642093284708, w0=72.67899622636382, w1=15.995831058380947\n",
      "SubGD iter. 372/499: loss=5.3111348365833395, w0=72.6264107329323, w1=15.979699626040832\n",
      "SubGD iter. 373/499: loss=5.310592204589957, w0=72.6264107329323, w1=15.967487837930207\n",
      "SubGD iter. 374/499: loss=5.31060665271566, w0=72.67899099682057, w1=16.001389990681908\n",
      "SubGD iter. 375/499: loss=5.3111516688696465, w0=72.62640533673276, w1=15.985258507217335\n",
      "SubGD iter. 376/499: loss=5.310604987322362, w0=72.62640533673276, w1=15.973046689712621\n",
      "SubGD iter. 377/499: loss=5.310576906060976, w0=72.62640533673276, w1=15.960834936781199\n",
      "SubGD iter. 378/499: loss=5.310649177312069, w0=72.67898602165664, w1=15.994737361003837\n",
      "SubGD iter. 379/499: loss=5.311131413675551, w0=72.6264005621153, w1=15.97860593906007\n",
      "SubGD iter. 380/499: loss=5.310589689639994, w0=72.6264005621153, w1=15.966394156732612\n",
      "SubGD iter. 381/499: loss=5.310613735321044, w0=72.67898089612838, w1=16.00029635469865\n",
      "SubGD iter. 382/499: loss=5.3111482471781795, w0=72.62639526991869, w1=15.984164881626732\n",
      "SubGD iter. 383/499: loss=5.310602472513566, w0=72.62639526991869, w1=15.971953069904862\n",
      "SubGD iter. 384/499: loss=5.310578300412999, w0=72.6789752530911, w1=16.005855041659522\n",
      "SubGD iter. 385/499: loss=5.311165074625318, w0=72.62638946027302, w1=15.989723517477843\n",
      "SubGD iter. 386/499: loss=5.310615254681841, w0=72.62638946027302, w1=15.97751167636318\n",
      "SubGD iter. 387/499: loss=5.310587173366165, w0=72.62638946027302, w1=15.965299899821932\n",
      "SubGD iter. 388/499: loss=5.310620830819769, w0=72.67896986453856, w1=15.999202143084622\n",
      "SubGD iter. 389/499: loss=5.31114481452155, w0=72.62638427231558, w1=15.98307068043866\n",
      "SubGD iter. 390/499: loss=5.310599956381177, w0=72.62638427231558, w1=15.970858874502673\n",
      "SubGD iter. 391/499: loss=5.31058539448699, w0=72.67896432572634, w1=16.004760891544887\n",
      "SubGD iter. 392/499: loss=5.31116164318741, w0=72.6263785668829, w1=15.988629377785465\n",
      "SubGD iter. 393/499: loss=5.31061273869086, w0=72.6263785668829, w1=15.976417542456362\n",
      "SubGD iter. 394/499: loss=5.310584657388488, w0=72.6263785668829, w1=15.964205771700644\n",
      "SubGD iter. 395/499: loss=5.310627923432437, w0=72.67895904137234, w1=15.998108060241558\n",
      "SubGD iter. 396/499: loss=5.311141384319634, w0=72.62637348311175, w1=15.981976608014099\n",
      "SubGD iter. 397/499: loss=5.31059744054488, w0=72.62637348311175, w1=15.969764807863317\n",
      "SubGD iter. 398/499: loss=5.3105924856756195, w0=72.67895360673229, w1=16.003666870174662\n",
      "SubGD iter. 399/499: loss=5.311158214203611, w0=72.62636788183919, w1=15.987535366830041\n",
      "SubGD iter. 400/499: loss=5.310610222995915, w0=72.62636788183919, w1=15.975323537285817\n",
      "SubGD iter. 401/499: loss=5.310582141706844, w0=72.62636788183919, w1=15.963111772314948\n",
      "SubGD iter. 402/499: loss=5.31063501316044, w0=72.67894842652395, w1=15.99701410611567\n",
      "SubGD iter. 403/499: loss=5.311137956571222, w0=72.62636290220146, w1=15.98088266429926\n",
      "SubGD iter. 404/499: loss=5.310594925004553, w0=72.62636290220146, w1=15.968670869933002\n",
      "SubGD iter. 405/499: loss=5.310599573980277, w0=72.67894309600325, w1=16.00257297749507\n",
      "SubGD iter. 406/499: loss=5.311154787672715, w0=72.62635740503619, w1=15.986441484557803\n",
      "SubGD iter. 407/499: loss=5.3106077075968745, w0=72.62635740503619, w1=15.974229660797777\n",
      "SubGD iter. 408/499: loss=5.310579626321105, w0=72.62635740503619, w1=15.962017901611075\n",
      "SubGD iter. 409/499: loss=5.310642100005169, w0=72.67893801988772, w1=15.995920280653198\n",
      "SubGD iter. 410/499: loss=5.311134531275101, w0=72.62635252947906, w1=15.979788849240391\n",
      "SubGD iter. 411/499: loss=5.310592409760071, w0=72.62635252947906, w1=15.967577060657975\n",
      "SubGD iter. 412/499: loss=5.310606659402351, w0=72.67893279343355, w1=16.001479213452363\n",
      "SubGD iter. 413/499: loss=5.311151363593508, w0=72.62634713636827, w1=15.985347730915\n",
      "SubGD iter. 414/499: loss=5.310605192493623, w0=72.62634713636827, w1=15.973135912938494\n",
      "SubGD iter. 415/499: loss=5.310577111231151, w0=72.62634713636827, w1=15.96092415953528\n",
      "SubGD iter. 416/499: loss=5.31064918396801, w0=72.67892782135804, w1=15.994826583800409\n",
      "SubGD iter. 417/499: loss=5.311131108430068, w0=72.62634236483893, w1=15.978695162783758\n",
      "SubGD iter. 418/499: loss=5.310589894811314, w0=72.62634236483893, w1=15.966483379984506\n",
      "SubGD iter. 419/499: loss=5.31061374194323, w0=72.67892269891757, w1=16.000385577992823\n",
      "SubGD iter. 420/499: loss=5.31114794196478, w0=72.62633707572981, w1=15.984254105847924\n",
      "SubGD iter. 421/499: loss=5.310602677686031, w0=72.62633707572981, w1=15.972042293654255\n",
      "SubGD iter. 422/499: loss=5.310578307001441, w0=72.67891705896744, w1=16.005944265450974\n",
      "SubGD iter. 423/499: loss=5.311164769443996, w0=72.62633126917096, w1=15.989812742196218\n",
      "SubGD iter. 424/499: loss=5.310615459855448, w0=72.62633126917096, w1=15.977600900609755\n",
      "SubGD iter. 425/499: loss=5.310587378538686, w0=72.62633126917096, w1=15.96538912359671\n",
      "SubGD iter. 426/499: loss=5.310620837377486, w0=72.67891167350143, w1=15.999291366901263\n",
      "SubGD iter. 427/499: loss=5.311144509370863, w0=72.62632608429973, w1=15.98315990518213\n",
      "SubGD iter. 428/499: loss=5.310600161554841, w0=72.62632608429973, w1=15.970948098774345\n",
      "SubGD iter. 429/499: loss=5.3105854010109805, w0=72.67890613777509, w1=16.004850115858204\n",
      "SubGD iter. 430/499: loss=5.311161338068787, w0=72.62632038195262, w1=15.988718603025514\n",
      "SubGD iter. 431/499: loss=5.310612943865668, w0=72.62632038195262, w1=15.976506767224608\n",
      "SubGD iter. 432/499: loss=5.3105848625622105, w0=72.62632038195262, w1=15.96429499599709\n",
      "SubGD iter. 433/499: loss=5.310627929925717, w0=72.67890085650635, w1=15.998197284579454\n",
      "SubGD iter. 434/499: loss=5.311141079231631, w0=72.62631530126643, w1=15.982065833278634\n",
      "SubGD iter. 435/499: loss=5.310597645719742, w0=72.62631530126643, w1=15.969854032656048\n",
      "SubGD iter. 436/499: loss=5.31059249213519, w0=72.67889542495094, w1=16.00375609500863\n",
      "SubGD iter. 437/499: loss=5.311157909147658, w0=72.62630970307819, w1=15.987624592590551\n",
      "SubGD iter. 438/499: loss=5.310610428171917, w0=72.62630970307819, w1=15.975412762574521\n",
      "SubGD iter. 439/499: loss=5.31058234688176, w0=72.62630970307819, w1=15.963200997131848\n",
      "SubGD iter. 440/499: loss=5.310635019589317, w0=72.6788902478266, w1=15.99710333097361\n",
      "SubGD iter. 441/499: loss=5.311137651545873, w0=72.62630472652417, w1=15.98097189008365\n",
      "SubGD iter. 442/499: loss=5.3105951301806105, w0=72.62630472652417, w1=15.968760095245585\n",
      "SubGD iter. 443/499: loss=5.310599580375458, w0=72.67888492038927, w1=16.00266220284848\n",
      "SubGD iter. 444/499: loss=5.3111544826794, w0=72.62629923244195, w1=15.986530710837563\n",
      "SubGD iter. 445/499: loss=5.3106079127740715, w0=72.62629923244195, w1=15.97431888660573\n",
      "SubGD iter. 446/499: loss=5.310579831497216, w0=72.62629923244195, w1=15.962107126947222\n",
      "SubGD iter. 447/499: loss=5.3106421063696745, w0=72.6788798473565, w1=15.996009506029976\n",
      "SubGD iter. 448/499: loss=5.31113422631238, w0=72.62629435996726, w1=15.979878075543429\n",
      "SubGD iter. 449/499: loss=5.310592614937323, w0=72.62629435996726, w1=15.967666286489205\n",
      "SubGD iter. 450/499: loss=5.310606665733177, w0=72.67887462398443, w1=16.001568439324007\n",
      "SubGD iter. 451/499: loss=5.311151058662806, w0=72.62628896993826, w1=15.985436957712807\n",
      "SubGD iter. 452/499: loss=5.310605397672011, w0=72.62628896993826, w1=15.973225139264489\n",
      "SubGD iter. 453/499: loss=5.3105773164084535, w0=72.62628896993826, w1=15.961013385389467\n",
      "SubGD iter. 454/499: loss=5.310649190268176, w0=72.67886965499042, w1=15.994915809694815\n",
      "SubGD iter. 455/499: loss=5.311130803529947, w0=72.62628420149012, w1=15.978784389604234\n",
      "SubGD iter. 456/499: loss=5.310590099989755, w0=72.62628420149012, w1=15.966572606333171\n",
      "SubGD iter. 457/499: loss=5.310613748209733, w0=72.67886453563081, w1=16.00047480438149\n",
      "SubGD iter. 458/499: loss=5.311147637096662, w0=72.62627891546154, w1=15.984343333162562\n",
      "SubGD iter. 459/499: loss=5.310602882865607, w0=72.62627891546154, w1=15.97213152049708\n",
      "SubGD iter. 460/499: loss=5.310578313234288, w0=72.6788588987609, w1=16.00603349233359\n",
      "SubGD iter. 461/499: loss=5.311164464607875, w0=72.6262731119826, w1=15.989901970004707\n",
      "SubGD iter. 462/499: loss=5.310615665036161, w0=72.6262731119826, w1=15.977690127946428\n",
      "SubGD iter. 463/499: loss=5.310587583718312, w0=72.6262731119826, w1=15.965478350461568\n",
      "SubGD iter. 464/499: loss=5.310620843579698, w0=72.67885351637447, w1=15.999380593805714\n",
      "SubGD iter. 465/499: loss=5.3111442045652995, w0=72.62626793019066, w1=15.983249133012364\n",
      "SubGD iter. 466/499: loss=5.3106003667356045, w0=72.62626793019066, w1=15.971037326132763\n",
      "SubGD iter. 467/499: loss=5.310585407179554, w0=72.6788479837271, w1=16.004939343256\n",
      "SubGD iter. 468/499: loss=5.311161033295207, w0=72.6262622309222, w1=15.988807831348996\n",
      "SubGD iter. 469/499: loss=5.310613149047564, w0=72.6262622309222, w1=15.976595995076272\n",
      "SubGD iter. 470/499: loss=5.31058506774302, w0=72.6262622309222, w1=15.964384223376937\n",
      "SubGD iter. 471/499: loss=5.310627936063674, w0=72.67884270553668, w1=15.998286511998487\n",
      "SubGD iter. 472/499: loss=5.311140774488593, w0=72.62625715331401, w1=15.982155061623258\n",
      "SubGD iter. 473/499: loss=5.310597850901687, w0=72.62625715331401, w1=15.969943260528854\n",
      "SubGD iter. 474/499: loss=5.310592498239522, w0=72.67883727705896, w1=16.003845322920405\n",
      "SubGD iter. 475/499: loss=5.311157604436589, w0=72.62625155820315, w1=15.987713821427821\n",
      "SubGD iter. 476/499: loss=5.310610633354995, w0=72.62625155820315, w1=15.97550199093997\n",
      "SubGD iter. 477/499: loss=5.310582552063754, w0=72.62625155820315, w1=15.963290225025478\n",
      "SubGD iter. 478/499: loss=5.310635025663048, w0=72.6788321030117, w1=15.997192558906015\n",
      "SubGD iter. 479/499: loss=5.311137346865334, w0=72.6262465847259, w1=15.981061118941456\n",
      "SubGD iter. 480/499: loss=5.310595335363735, w0=72.6262465847259, w1=15.96884932363157\n",
      "SubGD iter. 481/499: loss=5.310599586415582, w0=72.67882677865082, w1=16.002751431273023\n",
      "SubGD iter. 482/499: loss=5.311154178030816, w0=72.62624109371981, w1=15.986619940187412\n",
      "SubGD iter. 483/499: loss=5.310608117958329, w0=72.62624109371981, w1=15.974408115483753\n",
      "SubGD iter. 484/499: loss=5.310580036680389, w0=72.62624109371981, w1=15.962196355353424\n",
      "SubGD iter. 485/499: loss=5.310642112379212, w0=72.67882170869386, w1=15.996098734474542\n",
      "SubGD iter. 486/499: loss=5.311133921694311, w0=72.62623622432065, w1=15.979967304913206\n",
      "SubGD iter. 487/499: loss=5.310592820121625, w0=72.62623622432065, w1=15.967755515387157\n",
      "SubGD iter. 488/499: loss=5.310606671709126, w0=72.67881648839698, w1=16.001657668260112\n",
      "SubGD iter. 489/499: loss=5.311150754076675, w0=72.6262308373665, w1=15.985526187574026\n",
      "SubGD iter. 490/499: loss=5.310605602857443, w0=72.6262308373665, w1=15.973314368653881\n",
      "SubGD iter. 491/499: loss=5.310577521592802, w0=72.6262308373665, w1=15.961102614307034\n",
      "SubGD iter. 492/499: loss=5.310649196213555, w0=72.67881152247753, w1=15.995005038650337\n",
      "SubGD iter. 493/499: loss=5.311130498974316, w0=72.62622607199263, w1=15.978873619484778\n",
      "SubGD iter. 494/499: loss=5.310590305175231, w0=72.62622607199263, w1=15.966661835741888\n",
      "SubGD iter. 495/499: loss=5.310613754121539, w0=72.67880640619185, w1=16.00056403382795\n",
      "SubGD iter. 496/499: loss=5.311147332572959, w0=72.62622078903766, w1=15.984432563533947\n",
      "SubGD iter. 497/499: loss=5.310603088052212, w0=72.62622078903766, w1=15.972220750396634\n",
      "SubGD iter. 498/499: loss=5.310578319112527, w0=72.6788007723952, w1=16.006122722270668\n",
      "SubGD iter. 499/499: loss=5.311164160116089, w0=72.62621498863169, w1=15.989991200866614\n",
      "SubGD: execution time=0.028 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c384bbc47f439eb2ead5390789112b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses, subgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "\n",
    "        # ***************************************************\n",
    "        # implement stochastic subgradient descent.\n",
    "        # compute subgradient and loss\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        subgradient = compute_subgradient_mae(y, tx, w)\n",
    "        \n",
    "        # update w by subgradient\n",
    "        w = w - subgradient * loss\n",
    "        # ***************************************************\n",
    "\n",
    "        \n",
    "        print(\"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=74.06780585492638, w0=74.06780585492638, w1=9.265323718695322e-14\n",
      "SubSGD iter. 1/499: loss=13.561731565085076, w0=73.66498214507236, w1=9.511349913666642\n",
      "SubSGD iter. 2/499: loss=7.416634966318797, w0=73.59155011570286, w1=13.501850638547138\n",
      "SubSGD iter. 3/499: loss=5.635119664315051, w0=73.36837705969039, w1=15.188374846791081\n",
      "SubSGD iter. 4/499: loss=5.368618587503157, w0=72.94313994384855, w1=15.502563495897409\n",
      "SubSGD iter. 5/499: loss=5.332294283084711, w0=72.78475496514307, w1=15.793972557729766\n",
      "SubSGD iter. 6/499: loss=5.312983046843682, w0=72.52173600242803, w1=15.983707637603938\n",
      "SubSGD iter. 7/499: loss=5.313206731511542, w0=72.67955402415609, w1=16.010522061356525\n",
      "SubSGD iter. 8/499: loss=5.311184980102729, w0=72.62696803425408, w1=15.99439047671623\n",
      "SubSGD iter. 9/499: loss=5.310625986426089, w0=72.62696803425408, w1=15.982178610923757\n",
      "SubSGD iter. 10/499: loss=5.3105979050536645, w0=72.62696803425408, w1=15.96996680970483\n",
      "SubSGD iter. 11/499: loss=5.310585309479802, w0=72.67954808682319, w1=16.003868826204368\n",
      "SubSGD iter. 12/499: loss=5.311164713543209, w0=72.62696229758019, w1=15.987737303119399\n",
      "SubSGD iter. 13/499: loss=5.310610687351503, w0=72.62696229758019, w1=15.975525472507382\n",
      "SubSGD iter. 14/499: loss=5.3105826060599775, w0=72.62696229758019, w1=15.963313706468725\n",
      "SubSGD iter. 15/499: loss=5.31062783873734, w0=72.67954277123106, w1=15.997215994468956\n",
      "SubSGD iter. 16/499: loss=5.31114445436483, w0=72.62695718257397, w1=15.981084532916894\n",
      "SubSGD iter. 17/499: loss=5.310595389204531, w0=72.62695718257397, w1=15.968872737483201\n",
      "SubSGD iter. 18/499: loss=5.310592401322646, w0=72.67953730535935, w1=16.00277479925605\n",
      "SubSGD iter. 19/499: loss=5.311161283923955, w0=72.62695155007297, w1=15.986643286587812\n",
      "SubSGD iter. 20/499: loss=5.310608171643733, w0=72.62695155007297, w1=15.974431461760703\n",
      "SubSGD iter. 21/499: loss=5.310580090365509, w0=72.62695155007297, w1=15.962219701506925\n",
      "SubSGD iter. 22/499: loss=5.310634929119391, w0=72.67953209392564, w1=15.99612203477114\n",
      "SubSGD iter. 23/499: loss=5.311141025981108, w0=72.62694653921297, w1=15.979990583632057\n",
      "SubSGD iter. 24/499: loss=5.3105928736513945, w0=72.62694653921297, w1=15.967778793982916\n",
      "SubSGD iter. 25/499: loss=5.310599490281186, w0=72.67952673218605, w1=16.001680901010662\n",
      "SubSGD iter. 26/499: loss=5.311157856757892, w0=72.62694101083201, w1=15.985549398751706\n",
      "SubSGD iter. 27/499: loss=5.3106056562319015, w0=72.62694101083201, w1=15.973337579708826\n",
      "SubSGD iter. 28/499: loss=5.310577574966977, w0=72.62694101083201, w1=15.961125825239245\n",
      "SubSGD iter. 29/499: loss=5.3106420166178365, w0=72.67952162485793, w1=15.995028203749037\n",
      "SubSGD iter. 30/499: loss=5.31113760004997, w0=72.62693610406535, w1=15.978896763015484\n",
      "SubSGD iter. 31/499: loss=5.310590358394135, w0=72.62693610406535, w1=15.966684979150216\n",
      "SubSGD iter. 32/499: loss=5.310606576356811, w0=72.67951636719759, w1=16.000587131414452\n",
      "SubSGD iter. 33/499: loss=5.311154432043811, w0=72.62693067975161, w1=15.98445563955733\n",
      "SubSGD iter. 34/499: loss=5.310603141115882, w0=72.62693067975161, w1=15.972243826297998\n",
      "SubSGD iter. 35/499: loss=5.3105750598642585, w0=72.62693067975161, w1=15.960032077611933\n",
      "SubSGD iter. 36/499: loss=5.310649101234062, w0=72.67951136392224, w1=15.9939345013489\n",
      "SubSGD iter. 37/499: loss=5.3111341765702065, w0=72.62692587702551, w1=15.977803071013433\n",
      "SubSGD iter. 38/499: loss=5.310587843432625, w0=72.62692587702551, w1=15.965591292931357\n",
      "SubSGD iter. 39/499: loss=5.31061365955091, w0=72.6795062102884, w1=15.999493490413693\n",
      "SubSGD iter. 40/499: loss=5.311151009780499, w0=72.6269205567262, w1=15.983362008950962\n",
      "SubSGD iter. 41/499: loss=5.310600626295552, w0=72.6269205567262, w1=15.971150201474497\n",
      "SubSGD iter. 42/499: loss=5.31057822495066, w0=72.67950053915146, w1=16.005052172747416\n",
      "SubSGD iter. 43/499: loss=5.31116783693537, w0=72.62691471898378, w1=15.988920640175813\n",
      "SubSGD iter. 44/499: loss=5.310613408453189, w0=72.62691471898378, w1=15.97670880330658\n",
      "SubSGD iter. 45/499: loss=5.310585327147274, w0=72.62691471898378, w1=15.964497031010742\n",
      "SubSGD iter. 46/499: loss=5.310620755638048, w0=72.67949512250495, w1=15.998399273793481\n",
      "SubSGD iter. 47/499: loss=5.311147576552292, w0=72.62690950293512, w1=15.982267802758445\n",
      "SubSGD iter. 48/499: loss=5.310598110151654, w0=72.62690950293512, w1=15.970056001067892\n",
      "SubSGD iter. 49/499: loss=5.310585319612916, w0=72.67948955560456, w1=16.00395801763212\n",
      "SubSGD iter. 50/499: loss=5.311164404926015, w0=72.62690376941717, w1=15.987826495484507\n",
      "SubSGD iter. 51/499: loss=5.310610892450714, w0=72.62690376941717, w1=15.975614664400862\n",
      "SubSGD iter. 52/499: loss=5.310582811158102, w0=72.62690376941717, w1=15.963402897890578\n",
      "SubSGD iter. 53/499: loss=5.310627848838832, w0=72.67948424316805, w1=15.997305185955296\n",
      "SubSGD iter. 54/499: loss=5.311144145779058, w0=72.62689865756627, w1=15.981173725340497\n",
      "SubSGD iter. 55/499: loss=5.310595594303875, w0=72.62689865756627, w1=15.968961929435174\n",
      "SubSGD iter. 56/499: loss=5.3105924113895115, w0=72.67947878045132, w1=16.00286399127229\n",
      "SubSGD iter. 57/499: loss=5.31116097537103, w0=72.62689302821992, w1=15.986732479541216\n",
      "SubSGD iter. 58/499: loss=5.310608376744296, w0=72.62689302821992, w1=15.974520654242475\n",
      "SubSGD iter. 59/499: loss=5.310580295464987, w0=72.62689302821992, w1=15.962308893517067\n",
      "SubSGD iter. 60/499: loss=5.310634939154653, w0=72.67947357217196, w1=15.996211226845347\n",
      "SubSGD iter. 61/499: loss=5.3111407174595895, w0=72.62688802051395, w1=15.980079776643331\n",
      "SubSGD iter. 62/499: loss=5.310593078752091, w0=72.62688802051395, w1=15.967867986522558\n",
      "SubSGD iter. 63/499: loss=5.310599500281839, w0=72.67946821358605, w1=16.001770093614144\n",
      "SubSGD iter. 64/499: loss=5.311157548269207, w0=72.62688249528635, w1=15.985638592292156\n",
      "SubSGD iter. 65/499: loss=5.310605861333814, w0=72.62688249528635, w1=15.973426772777641\n",
      "SubSGD iter. 66/499: loss=5.3105777800678045, w0=72.62688249528635, w1=15.961215017836427\n",
      "SubSGD iter. 67/499: loss=5.3106420265869, w0=72.67946310941097, w1=15.995117396409858\n",
      "SubSGD iter. 68/499: loss=5.311137291592676, w0=72.62687759167244, w1=15.97898595661318\n",
      "SubSGD iter. 69/499: loss=5.310590563496178, w0=72.62687759167244, w1=15.966774172276276\n",
      "SubSGD iter. 70/499: loss=5.310606586291283, w0=72.67945785490305, w1=16.000676324603933\n",
      "SubSGD iter. 71/499: loss=5.311154123619334, w0=72.62687217051078, w1=15.984544833683586\n",
      "SubSGD iter. 72/499: loss=5.31060334621914, w0=72.62687217051078, w1=15.972333019952615\n",
      "SubSGD iter. 73/499: loss=5.310575264966433, w0=72.62687217051078, w1=15.960121270794914\n",
      "SubSGD iter. 74/499: loss=5.310649111136963, w0=72.67945285477946, w1=15.9940236945951\n",
      "SubSGD iter. 75/499: loss=5.311133868177109, w0=72.62686737093611, w1=15.97789226519631\n",
      "SubSGD iter. 76/499: loss=5.310588048536015, w0=72.62686737093611, w1=15.965680486642595\n",
      "SubSGD iter. 77/499: loss=5.310613669419236, w0=72.67944770429669, w1=15.999582684187928\n",
      "SubSGD iter. 78/499: loss=5.311150701420204, w0=72.62686205378758, w1=15.983451203661774\n",
      "SubSGD iter. 79/499: loss=5.310600831400156, w0=72.62686205378758, w1=15.971239395713669\n",
      "SubSGD iter. 80/499: loss=5.31057823478442, w0=72.67944203631019, w1=16.005141367049365\n",
      "SubSGD iter. 81/499: loss=5.311167528607869, w0=72.62685621919526, w1=15.98900983541424\n",
      "SubSGD iter. 82/499: loss=5.310613613559006, w0=72.62685621919526, w1=15.976797998073364\n",
      "SubSGD iter. 83/499: loss=5.310585532252007, w0=72.62685621919526, w1=15.964586225305883\n",
      "SubSGD iter. 84/499: loss=5.310620765440261, w0=72.67943662281348, w1=15.998488468151198\n",
      "SubSGD iter. 85/499: loss=5.311147268256146, w0=72.62685100629609, w1=15.982356998052545\n",
      "SubSGD iter. 86/499: loss=5.310598315257599, w0=72.62685100629609, w1=15.970145195890348\n",
      "SubSGD iter. 87/499: loss=5.310585329380581, w0=72.67943105906224, w1=16.00404721251693\n",
      "SubSGD iter. 88/499: loss=5.311164096662651, w0=72.62684527592697, w1=15.987915691305602\n",
      "SubSGD iter. 89/499: loss=5.310611097557871, w0=72.62684527592697, w1=15.97570385975031\n",
      "SubSGD iter. 90/499: loss=5.310583016264175, w0=72.62684527592697, w1=15.963492092768382\n",
      "SubSGD iter. 91/499: loss=5.310627858574966, w0=72.67942574977424, w1=15.997394380895255\n",
      "SubSGD iter. 92/499: loss=5.311143837547035, w0=72.62684016722427, w1=15.981262921216643\n",
      "SubSGD iter. 93/499: loss=5.31059579941116, w0=72.62684016722427, w1=15.969051124839673\n",
      "SubSGD iter. 94/499: loss=5.3105924210911155, w0=72.67942029020537, w1=16.002953186738722\n",
      "SubSGD iter. 95/499: loss=5.311160667171773, w0=72.62683454102546, w1=15.986821675943737\n",
      "SubSGD iter. 96/499: loss=5.31060858185279, w0=72.62683454102546, w1=15.974609850173346\n",
      "SubSGD iter. 97/499: loss=5.310580500572397, w0=72.62683454102546, w1=15.96239808897629\n",
      "SubSGD iter. 98/499: loss=5.310634948824742, w0=72.67941508507323, w1=15.996300422366302\n",
      "SubSGD iter. 99/499: loss=5.311140409291659, w0=72.62682953646639, w1=15.98016897310028\n",
      "SubSGD iter. 100/499: loss=5.31059328386071, w0=72.62682953646639, w1=15.967957182507856\n",
      "SubSGD iter. 101/499: loss=5.310599509917411, w0=72.67940972963389, w1=16.001859289660956\n",
      "SubSGD iter. 102/499: loss=5.311157240134027, w0=72.62682401438504, w1=15.985727789274863\n",
      "SubSGD iter. 103/499: loss=5.310606066443641, w0=72.62682401438504, w1=15.973515969288695\n",
      "SubSGD iter. 104/499: loss=5.31057798517655, w0=72.62682401438504, w1=15.96130421387583\n",
      "SubSGD iter. 105/499: loss=5.310642036190975, w0=72.67940462860474, w1=15.995206592510572\n",
      "SubSGD iter. 106/499: loss=5.311136983488807, w0=72.62681911391674, w1=15.979075153649692\n",
      "SubSGD iter. 107/499: loss=5.310590768606132, w0=72.62681911391674, w1=15.966863368841135\n",
      "SubSGD iter. 108/499: loss=5.31060659586086, w0=72.67939937724209, w1=16.000765521229884\n",
      "SubSGD iter. 109/499: loss=5.311153815548201, w0=72.62681369590003, w1=15.984634031245236\n",
      "SubSGD iter. 110/499: loss=5.310603551330301, w0=72.62681369590003, w1=15.972422217042608\n",
      "SubSGD iter. 111/499: loss=5.310575470076507, w0=72.62681369590003, w1=15.960210467413255\n",
      "SubSGD iter. 112/499: loss=5.310649120675061, w0=72.67939438026315, w1=15.99411289127433\n",
      "SubSGD iter. 113/499: loss=5.311133560137274, w0=72.62680889946971, w1=15.977981462811146\n",
      "SubSGD iter. 114/499: loss=5.310588253647296, w0=72.62680889946971, w1=15.965769683785775\n",
      "SubSGD iter. 115/499: loss=5.31061367892285, w0=72.6793892329244, w1=15.999671881391777\n",
      "SubSGD iter. 116/499: loss=5.311150393413092, w0=72.62680358546486, w1=15.983540401801129\n",
      "SubSGD iter. 117/499: loss=5.310601036512644, w0=72.62680358546486, w1=15.971328593381363\n",
      "SubSGD iter. 118/499: loss=5.310578244253562, w0=72.67938356808122, w1=16.00523056477751\n",
      "SubSGD iter. 119/499: loss=5.311167220633472, w0=72.62679775401554, w1=15.98909903407779\n",
      "SubSGD iter. 120/499: loss=5.3106138186727, w0=72.62679775401554, w1=15.976887196265253\n",
      "SubSGD iter. 121/499: loss=5.310585737364615, w0=72.62679775401554, w1=15.964675423026112\n",
      "SubSGD iter. 122/499: loss=5.310620774877947, w0=72.6793781577272, w1=15.998577665931675\n",
      "SubSGD iter. 123/499: loss=5.311146960313024, w0=72.62679254425875, w1=15.982446196768333\n",
      "SubSGD iter. 124/499: loss=5.310598520371412, w0=72.62679254425875, w1=15.970234394134472\n",
      "SubSGD iter. 125/499: loss=5.3105853387838104, w0=72.67937259711799, w1=16.004136410821083\n",
      "SubSGD iter. 126/499: loss=5.311163788752227, w0=72.62678681703133, w1=15.988004890544968\n",
      "SubSGD iter. 127/499: loss=5.310611302672888, w0=72.62678681703133, w1=15.97579305851801\n",
      "SubSGD iter. 128/499: loss=5.310583221378107, w0=72.62678681703133, w1=15.963581291064418\n",
      "SubSGD iter. 129/499: loss=5.310627867946759, w0=72.6793672909714, w1=15.997483579251119\n",
      "SubSGD iter. 130/499: loss=5.311143529667872, w0=72.62678171146973, w1=15.981352120507625\n",
      "SubSGD iter. 131/499: loss=5.310596004526295, w0=72.62678171146973, w1=15.969140323658989\n",
      "SubSGD iter. 132/499: loss=5.310592430428465, w0=72.67936183454329, w1=16.003042385617643\n",
      "SubSGD iter. 133/499: loss=5.311160359325295, w0=72.62677608841135, w1=15.986910875757676\n",
      "SubSGD iter. 134/499: loss=5.310608786969127, w0=72.62677608841135, w1=15.974699049515618\n",
      "SubSGD iter. 135/499: loss=5.310580705687651, w0=72.62677608841135, w1=15.962487287846896\n",
      "SubSGD iter. 136/499: loss=5.310634958130672, w0=72.67935663255126, w1=15.996389621296316\n",
      "SubSGD iter. 137/499: loss=5.311140101476426, w0=72.6267710869921, w1=15.980258172965218\n",
      "SubSGD iter. 138/499: loss=5.310593488977165, w0=72.6267710869921, w1=15.968046381901127\n",
      "SubSGD iter. 139/499: loss=5.310599519188918, w0=72.6793512802514, w1=16.001948489113413\n",
      "SubSGD iter. 140/499: loss=5.311156932351464, w0=72.62676556804989, w1=15.985816989662144\n",
      "SubSGD iter. 141/499: loss=5.310606271561298, w0=72.62676556804989, w1=15.973605169204305\n",
      "SubSGD iter. 142/499: loss=5.31057819029312, w0=72.62676556804989, w1=15.96139341331977\n",
      "SubSGD iter. 143/499: loss=5.310642045431079, w0=72.67934618236109, w1=15.995295792013502\n",
      "SubSGD iter. 144/499: loss=5.311136675737476, w0=72.62676067072012, w1=15.97916435408735\n",
      "SubSGD iter. 145/499: loss=5.310590973723902, w0=72.62676067072012, w1=15.966952568807121\n",
      "SubSGD iter. 146/499: loss=5.310606605066555, w0=72.67934093413662, w1=16.000854721254637\n",
      "SubSGD iter. 147/499: loss=5.311153507829528, w0=72.62675525584127, w1=15.984723232204617\n",
      "SubSGD iter. 148/499: loss=5.3106037564492725, w0=72.62675525584127, w1=15.972511417530315\n",
      "SubSGD iter. 149/499: loss=5.310575675194395, w0=72.62675525584127, w1=15.96029966742929\n",
      "SubSGD iter. 150/499: loss=5.310649129849369, w0=72.67933594029523, w1=15.994202091348933\n",
      "SubSGD iter. 151/499: loss=5.311133252449815, w0=72.6267504625482, w1=15.978070663820283\n",
      "SubSGD iter. 152/499: loss=5.310588458766383, w0=72.6267504625482, w1=15.965858884323238\n",
      "SubSGD iter. 153/499: loss=5.310613688062769, w0=72.67933079609338, w1=15.999761081987588\n",
      "SubSGD iter. 154/499: loss=5.311150085758275, w0=72.62674515167993, w1=15.983629603331377\n",
      "SubSGD iter. 155/499: loss=5.31060124163293, w0=72.62674515167993, w1=15.971417794439933\n",
      "SubSGD iter. 156/499: loss=5.310578253359098, w0=72.67932513438646, w1=16.005319765894207\n",
      "SubSGD iter. 157/499: loss=5.311166913011288, w0=72.62673932336655, w1=15.989188236128825\n",
      "SubSGD iter. 158/499: loss=5.310614023794182, w0=72.62673932336655, w1=15.976976397844608\n",
      "SubSGD iter. 159/499: loss=5.3105859424850115, w0=72.62673932336655, w1=15.96476462413379\n",
      "SubSGD iter. 160/499: loss=5.310620783952122, w0=72.67931972716805, w1=15.998666867097283\n",
      "SubSGD iter. 161/499: loss=5.311146652722035, w0=72.62673411674506, w1=15.982535398868182\n",
      "SubSGD iter. 162/499: loss=5.310598725493008, w0=72.62673411674506, w1=15.970323595762641\n",
      "SubSGD iter. 163/499: loss=5.3105853478236185, w0=72.67931416969381, w1=16.00422561250696\n",
      "SubSGD iter. 164/499: loss=5.311163481193857, w0=72.62672839265228, w1=15.988094093164987\n",
      "SubSGD iter. 165/499: loss=5.310611507795678, w0=72.62672839265228, w1=15.975882260666346\n",
      "SubSGD iter. 166/499: loss=5.310583426499813, w0=72.62672839265228, w1=15.963670492741075\n",
      "SubSGD iter. 167/499: loss=5.310627876955221, w0=72.67930886668154, w1=15.997572780985283\n",
      "SubSGD iter. 168/499: loss=5.311143222140681, w0=72.6267232902247, w1=15.981441323175837\n",
      "SubSGD iter. 169/499: loss=5.310596209649196, w0=72.6267232902247, w1=15.969229525855518\n",
      "SubSGD iter. 170/499: loss=5.31059243940258, w0=72.6793034133871, w1=16.00313158787146\n",
      "SubSGD iter. 171/499: loss=5.31116005183071, w0=72.62671767029967, w1=15.987000078945444\n",
      "SubSGD iter. 172/499: loss=5.310608992093226, w0=72.62671767029967, w1=15.974788252231699\n",
      "SubSGD iter. 173/499: loss=5.310580910810661, w0=72.62671767029967, w1=15.962576490091294\n",
      "SubSGD iter. 174/499: loss=5.310634967073455, w0=72.67929821452812, w1=15.996478823597803\n",
      "SubSGD iter. 175/499: loss=5.311139794013005, w0=72.62671267201314, w1=15.980347376200559\n",
      "SubSGD iter. 176/499: loss=5.310593694101372, w0=72.62671267201314, w1=15.96813558466478\n",
      "SubSGD iter. 177/499: loss=5.31059952809737, w0=72.67929286536064, w1=16.00203769193394\n",
      "SubSGD iter. 178/499: loss=5.311156624920635, w0=72.62670715620301, w1=15.985906193416424\n",
      "SubSGD iter. 179/499: loss=5.310606476686698, w0=72.62670715620301, w1=15.973694372486895\n",
      "SubSGD iter. 180/499: loss=5.310578395417435, w0=72.62670715620301, w1=15.961482616130676\n",
      "SubSGD iter. 181/499: loss=5.31064205430822, w0=72.6792877706021, w1=15.995384994881077\n",
      "SubSGD iter. 182/499: loss=5.311136368337796, w0=72.6267022620047, w1=15.979253557888585\n",
      "SubSGD iter. 183/499: loss=5.3105911788494105, w0=72.6267022620047, w1=15.967041772136668\n",
      "SubSGD iter. 184/499: loss=5.310606613909381, w0=72.67928252550875, w1=16.000943924640634\n",
      "SubSGD iter. 185/499: loss=5.3111532004624245, w0=72.62669685025665, w1=15.984812436524177\n",
      "SubSGD iter. 186/499: loss=5.310603961575973, w0=72.62669685025665, w1=15.972600621378183\n",
      "SubSGD iter. 187/499: loss=5.310575880320009, w0=72.62669685025665, w1=15.960388870805467\n",
      "SubSGD iter. 188/499: loss=5.310649138660898, w0=72.67927753479785, w1=15.994291294781362\n",
      "SubSGD iter. 189/499: loss=5.3111329451138465, w0=72.62669206009375, w1=15.97815986818618\n",
      "SubSGD iter. 190/499: loss=5.3105886638931885, w0=72.62669206009375, w1=15.965948088217443\n",
      "SubSGD iter. 191/499: loss=5.31061369684, w0=72.67927239372582, w1=15.999850285937827\n",
      "SubSGD iter. 192/499: loss=5.311149778454868, w0=72.62668675235498, w1=15.983718808214983\n",
      "SubSGD iter. 193/499: loss=5.310601446760925, w0=72.62668675235498, w1=15.971506998851845\n",
      "SubSGD iter. 194/499: loss=5.310578262102038, w0=72.67926673514808, w1=16.005408970361934\n",
      "SubSGD iter. 195/499: loss=5.311166605740436, w0=72.62668092717045, w1=15.989277441529822\n",
      "SubSGD iter. 196/499: loss=5.310614228923367, w0=72.62668092717045, w1=15.977065602773907\n",
      "SubSGD iter. 197/499: loss=5.310586147613114, w0=72.62668092717045, w1=15.964853828591394\n",
      "SubSGD iter. 198/499: loss=5.310620792663792, w0=72.67926133105821, w1=15.9987560716105\n",
      "SubSGD iter. 199/499: loss=5.311146345482295, w0=72.6266757236772, w1=15.982624604314575\n",
      "SubSGD iter. 200/499: loss=5.310598930622296, w0=72.6266757236772, w1=15.970412800737337\n",
      "SubSGD iter. 201/499: loss=5.310585356501014, w0=72.67925577671186, w1=16.004314817537054\n",
      "SubSGD iter. 202/499: loss=5.3111631739866585, w0=72.62667000271199, w1=15.988183299128156\n",
      "SubSGD iter. 203/499: loss=5.310611712926157, w0=72.62667000271199, w1=15.975971466157814\n",
      "SubSGD iter. 204/499: loss=5.310583631629208, w0=72.62667000271199, w1=15.963759697760844\n",
      "SubSGD iter. 205/499: loss=5.310627885601363, w0=72.67925047682685, w1=15.99766198606025\n",
      "SubSGD iter. 206/499: loss=5.311142914964581, w0=72.62666490341137, w1=15.981530529183784\n",
      "SubSGD iter. 207/499: loss=5.310596414779777, w0=72.62666490341137, w1=15.969318731391764\n",
      "SubSGD iter. 208/499: loss=5.310592448014465, w0=72.67924502665903, w1=16.003220793462688\n",
      "SubSGD iter. 209/499: loss=5.311159744687133, w0=72.62665928661262, w1=15.987089285469553\n",
      "SubSGD iter. 210/499: loss=5.310609197224994, w0=72.62665928661262, w1=15.974877458284105\n",
      "SubSGD iter. 211/499: loss=5.310581115941346, w0=72.62665928661262, w1=15.962665695672\n",
      "SubSGD iter. 212/499: loss=5.3106349756541045, w0=72.67923983092602, w1=15.996568029233286\n",
      "SubSGD iter. 213/499: loss=5.311139486900514, w0=72.62665429145176, w1=15.98043658276883\n",
      "SubSGD iter. 214/499: loss=5.3105938992332415, w0=72.62665429145176, w1=15.968224790761347\n",
      "SubSGD iter. 215/499: loss=5.310599536643778, w0=72.67923448488388, w1=16.002126898085063\n",
      "SubSGD iter. 216/499: loss=5.311156317840654, w0=72.62664877876665, w1=15.985995400500238\n",
      "SubSGD iter. 217/499: loss=5.310606681819752, w0=72.62664877876665, w1=15.973783579099003\n",
      "SubSGD iter. 218/499: loss=5.310578600549405, w0=72.62664877876665, w1=15.961571822271079\n",
      "SubSGD iter. 219/499: loss=5.310642062823407, w0=72.67922939325005, w1=15.99547420107584\n",
      "SubSGD iter. 220/499: loss=5.311136061288886, w0=72.62664388769274, w1=15.979342765015945\n",
      "SubSGD iter. 221/499: loss=5.310591383982565, w0=72.62664388769274, w1=15.967130978792321\n",
      "SubSGD iter. 222/499: loss=5.3106066223903445, w0=72.67922415128076, w1=16.00103313135043\n",
      "SubSGD iter. 223/499: loss=5.311152893446008, w0=72.62663847906843, w1=15.984901644166468\n",
      "SubSGD iter. 224/499: loss=5.310604166710311, w0=72.62663847906843, w1=15.972689828548766\n",
      "SubSGD iter. 225/499: loss=5.310576085453265, w0=72.62663847906843, w1=15.960478077504344\n",
      "SubSGD iter. 226/499: loss=5.310649147110658, w0=72.67921916369329, w1=15.994380501534181\n",
      "SubSGD iter. 227/499: loss=5.311132638128488, w0=72.62663369202865, w1=15.9782490758714\n",
      "SubSGD iter. 228/499: loss=5.310588869027627, w0=72.62663369202865, w1=15.966037295430953\n",
      "SubSGD iter. 229/499: loss=5.310613705255551, w0=72.67921402574405, w1=15.999939493205058\n",
      "SubSGD iter. 230/499: loss=5.311149471501988, w0=72.62662838741234, w1=15.983808016414518\n",
      "SubSGD iter. 231/499: loss=5.310601651896546, w0=72.62662838741234, w1=15.971596206579667\n",
      "SubSGD iter. 232/499: loss=5.31057827048339, w0=72.6792083702884, w1=16.00549817814326\n",
      "SubSGD iter. 233/499: loss=5.31116629882003, w0=72.62662256534959, w1=15.989366650243351\n",
      "SubSGD iter. 234/499: loss=5.310614434060169, w0=72.62662256534959, w1=15.97715481101572\n",
      "SubSGD iter. 235/499: loss=5.310586352748832, w0=72.62662256534959, w1=15.964943036361495\n",
      "SubSGD iter. 236/499: loss=5.3106208010139655, w0=72.67920296932003, w1=15.998845279433908\n",
      "SubSGD iter. 237/499: loss=5.3111460385929234, w0=72.62661736497752, w1=15.982713813070093\n",
      "SubSGD iter. 238/499: loss=5.310599135759197, w0=72.62661736497752, w1=15.97050200902114\n",
      "SubSGD iter. 239/499: loss=5.310585364817007, w0=72.67919741809452, w1=16.004404025873942\n",
      "SubSGD iter. 240/499: loss=5.311162867129744, w0=72.62661164713283, w1=15.988272508397058\n",
      "SubSGD iter. 241/499: loss=5.310611918064238, w0=72.62661164713283, w1=15.976060674954999\n",
      "SubSGD iter. 242/499: loss=5.310583836766202, w0=72.62661164713283, w1=15.963848906086314\n",
      "SubSGD iter. 243/499: loss=5.310627893886194, w0=72.67919212132972, w1=15.997751194438608\n",
      "SubSGD iter. 244/499: loss=5.311142608138686, w0=72.6266065509521, w1=15.98161973849406\n",
      "SubSGD iter. 245/499: loss=5.310596619917954, w0=72.6266065509521, w1=15.969407940230322\n",
      "SubSGD iter. 246/499: loss=5.310592456265129, w0=72.67918667428147, w1=16.003310002353917\n",
      "SubSGD iter. 247/499: loss=5.311159437893685, w0=72.62660093727261, w1=15.9871784952926\n",
      "SubSGD iter. 248/499: loss=5.310609402364347, w0=72.62660093727261, w1=15.97496666763543\n",
      "SubSGD iter. 249/499: loss=5.310581321079616, w0=72.62660093727261, w1=15.962754904551606\n",
      "SubSGD iter. 250/499: loss=5.310634983873624, w0=72.6791814816674, w1=15.996657238165364\n",
      "SubSGD iter. 251/499: loss=5.31113918013807, w0=72.62659594523039, w1=15.980525792632633\n",
      "SubSGD iter. 252/499: loss=5.310594104372689, w0=72.62659594523039, w1=15.96831400015343\n",
      "SubSGD iter. 253/499: loss=5.310599544829149, w0=72.67917613874356, w1=16.002216107529403\n",
      "SubSGD iter. 254/499: loss=5.31115601111064, w0=72.62659043566326, w1=15.986084610876205\n",
      "SubSGD iter. 255/499: loss=5.310606886960379, w0=72.62659043566326, w1=15.973872789003245\n",
      "SubSGD iter. 256/499: loss=5.310578805688946, w0=72.62659043566326, w1=15.9616610317036\n",
      "SubSGD iter. 257/499: loss=5.310642070977648, w0=72.6791710502274, w1=15.995563410560417\n",
      "SubSGD iter. 258/499: loss=5.311135754589862, w0=72.62658554770671, w1=15.979431975432053\n",
      "SubSGD iter. 259/499: loss=5.310591589123285, w0=72.62658554770671, w1=15.967220188736704\n",
      "SubSGD iter. 260/499: loss=5.310606630510452, w0=72.67916581137513, w1=16.00112234134665\n",
      "SubSGD iter. 261/499: loss=5.311152586779401, w0=72.62658014219909, w1=15.984990855094125\n",
      "SubSGD iter. 262/499: loss=5.3106043718522065, w0=72.62658014219909, w1=15.972779039004696\n",
      "SubSGD iter. 263/499: loss=5.310576290594074, w0=72.62658014219909, w1=15.96056728748855\n",
      "SubSGD iter. 264/499: loss=5.310649155199653, w0=72.67916082690404, w1=15.994469711570025\n",
      "SubSGD iter. 265/499: loss=5.311132331492854, w0=72.6265753582754, w1=15.978338286838584\n",
      "SubSGD iter. 266/499: loss=5.310589074169613, w0=72.6265753582754, w1=15.96612650592641\n",
      "SubSGD iter. 267/499: loss=5.310613713310431, w0=72.67915569207055, w1=16.000028703751937\n",
      "SubSGD iter. 268/499: loss=5.311149164898757, w0=72.62657005677453, w1=15.983897227892639\n",
      "SubSGD iter. 269/499: loss=5.310601857039706, w0=72.62657005677453, w1=15.971685417586059\n",
      "SubSGD iter. 270/499: loss=5.310578278504163, w0=72.67915003973002, w1=16.005587389200855\n",
      "SubSGD iter. 271/499: loss=5.311165992249192, w0=72.62656423782656, w1=15.98945586223209\n",
      "SubSGD iter. 272/499: loss=5.310614639204505, w0=72.62656423782656, w1=15.977244022532727\n",
      "SubSGD iter. 273/499: loss=5.310586557892082, w0=72.62656423782656, w1=15.965032247406771\n",
      "SubSGD iter. 274/499: loss=5.310620809003649, w0=72.67914464187609, w1=15.99893449053019\n",
      "SubSGD iter. 275/499: loss=5.31114573205304, w0=72.62655904056864, w1=15.982803025097423\n",
      "SubSGD iter. 276/499: loss=5.31059934090362, w0=72.62655904056864, w1=15.970591220576736\n",
      "SubSGD iter. 277/499: loss=5.310585372772602, w0=72.6791390937644, w1=16.00449323748033\n",
      "SubSGD iter. 278/499: loss=5.311162560622241, w0=72.62655332583745, w1=15.988361720934394\n",
      "SubSGD iter. 279/499: loss=5.310612123209833, w0=72.62655332583745, w1=15.976149887020599\n",
      "SubSGD iter. 280/499: loss=5.310584041910714, w0=72.62655332583745, w1=15.963938117680181\n",
      "SubSGD iter. 281/499: loss=5.310627901810714, w0=72.67913380011281, w1=15.997840406083064\n",
      "SubSGD iter. 282/499: loss=5.311142301662121, w0=72.62654823276962, w1=15.981708951069374\n",
      "SubSGD iter. 283/499: loss=5.310596825063636, w0=72.62654823276962, w1=15.9694971523339\n",
      "SubSGD iter. 284/499: loss=5.310592464155578, w0=72.6791283561771, w1=16.003399214507866\n",
      "SubSGD iter. 285/499: loss=5.311159131449484, w0=72.62654262220235, w1=15.987267708377308\n",
      "SubSGD iter. 286/499: loss=5.3106096075112035, w0=72.62654262220235, w1=15.9750558802484\n",
      "SubSGD iter. 287/499: loss=5.310581526225387, w0=72.62654262220235, w1=15.962844116692839\n",
      "SubSGD iter. 288/499: loss=5.310634991733017, w0=72.67912316667496, w1=15.99674645035677\n",
      "SubSGD iter. 289/499: loss=5.311138873724795, w0=72.62653763327174, w1=15.980615005754704\n",
      "SubSGD iter. 290/499: loss=5.31059430951963, w0=72.62653763327174, w1=15.968403212803763\n",
      "SubSGD iter. 291/499: loss=5.310599552654483, w0=72.67911782686238, w1=16.00230532022969\n",
      "SubSGD iter. 292/499: loss=5.311155704729714, w0=72.62653212681555, w1=15.986173824507059\n",
      "SubSGD iter. 293/499: loss=5.31060709210849, w0=72.62653212681555, w1=15.973962002162358\n",
      "SubSGD iter. 294/499: loss=5.3105790108359745, w0=72.62653212681555, w1=15.961750244390974\n",
      "SubSGD iter. 295/499: loss=5.310642078771943, w0=72.67911274145685, w1=15.995652623297548\n",
      "SubSGD iter. 296/499: loss=5.311135448239847, w0=72.62652724196933, w1=15.979521189099657\n",
      "SubSGD iter. 297/499: loss=5.310591794271478, w0=72.62652724196933, w1=15.967309401932567\n",
      "SubSGD iter. 298/499: loss=5.310606638270707, w0=72.67910750571458, w1=16.001211554592054\n",
      "SubSGD iter. 299/499: loss=5.31115228046172, w0=72.6265218395714, w1=15.985080069269902\n",
      "SubSGD iter. 300/499: loss=5.310604577001569, w0=72.6265218395714, w1=15.972868252708729\n",
      "SubSGD iter. 301/499: loss=5.310576495742352, w0=72.6265218395714, w1=15.960656500720841\n",
      "SubSGD iter. 302/499: loss=5.310649162928885, w0=72.67910252435287, w1=15.99455892485166\n",
      "SubSGD iter. 303/499: loss=5.311132025206071, w0=72.62651705875678, w1=15.978427501050499\n",
      "SubSGD iter. 304/499: loss=5.310589279319059, w0=72.62651705875678, w1=15.96621571966658\n",
      "SubSGD iter. 305/499: loss=5.310613721005639, w0=72.67909739262812, w1=16.000117917541235\n",
      "SubSGD iter. 306/499: loss=5.311148858644295, w0=72.62651176036432, w1=15.98398644261212\n",
      "SubSGD iter. 307/499: loss=5.31060206219032, w0=72.62651176036432, w1=15.971774631833792\n",
      "SubSGD iter. 308/499: loss=5.310578286165355, w0=72.67909174339566, w1=16.0056766034975\n",
      "SubSGD iter. 309/499: loss=5.311165686027044, w0=72.6265059445241, w1=15.989545077458818\n",
      "SubSGD iter. 310/499: loss=5.310614844356285, w0=72.6265059445241, w1=15.977333237287704\n",
      "SubSGD iter. 311/499: loss=5.310586763042777, w0=72.6265059445241, w1=15.965121461690002\n",
      "SubSGD iter. 312/499: loss=5.310620816633845, w0=72.67908634864918, w1=15.999023704862129\n",
      "SubSGD iter. 313/499: loss=5.311145425861766, w0=72.62650075037332, w1=15.982892240359353\n",
      "SubSGD iter. 314/499: loss=5.310599546055482, w0=72.62650075037332, w1=15.970680435366917\n",
      "SubSGD iter. 315/499: loss=5.310585380368796, w0=72.6790808036443, w1=16.004582452319003\n",
      "SubSGD iter. 316/499: loss=5.311162254463267, w0=72.62649503874862, w1=15.98845093670296\n",
      "SubSGD iter. 317/499: loss=5.3106123283628595, w0=72.62649503874862, w1=15.976239102317413\n",
      "SubSGD iter. 318/499: loss=5.310584247062655, w0=72.62649503874862, w1=15.964027332505244\n",
      "SubSGD iter. 319/499: loss=5.310627909375931, w0=72.67907551309887, w1=15.997929620956423\n",
      "SubSGD iter. 320/499: loss=5.311141995534007, w0=72.62648994878664, w1=15.981798166872531\n",
      "SubSGD iter. 321/499: loss=5.310597030216741, w0=72.62648994878664, w1=15.969586367665304\n",
      "SubSGD iter. 322/499: loss=5.31059247168681, w0=72.67907007226869, w1=16.003488429887348\n",
      "SubSGD iter. 323/499: loss=5.3111588253536555, w0=72.62648434132458, w1=15.987356924686491\n",
      "SubSGD iter. 324/499: loss=5.310609812665473, w0=72.62648434132458, w1=15.975145096085829\n",
      "SubSGD iter. 325/499: loss=5.31058173137857, w0=72.62648434132458, w1=15.962933332058515\n",
      "SubSGD iter. 326/499: loss=5.310634999233286, w0=72.67906488587145, w1=15.996835665770329\n",
      "SubSGD iter. 327/499: loss=5.31113856765981, w0=72.62647935549859, w1=15.980704222097868\n",
      "SubSGD iter. 328/499: loss=5.310594514673978, w0=72.62647935549859, w1=15.968492428675171\n",
      "SubSGD iter. 329/499: loss=5.310599560120784, w0=72.67905954916316, w1=16.002394536148763\n",
      "SubSGD iter. 330/499: loss=5.311155398697, w0=72.62647385214636, w1=15.98626304135564\n",
      "SubSGD iter. 331/499: loss=5.310607297263999, w0=72.62647385214636, w1=15.97405121853918\n",
      "SubSGD iter. 332/499: loss=5.310579215990397, w0=72.62647385214636, w1=15.96183946029604\n",
      "SubSGD iter. 333/499: loss=5.310642086207299, w0=72.67905446686129, w1=15.995741839250082\n",
      "SubSGD iter. 334/499: loss=5.311135142237964, w0=72.6264689704035, w1=15.979610405981607\n",
      "SubSGD iter. 335/499: loss=5.310591999427066, w0=72.6264689704035, w1=15.967398618342758\n",
      "SubSGD iter. 336/499: loss=5.310606645672111, w0=72.67904923422203, w1=16.001300771049493\n",
      "SubSGD iter. 337/499: loss=5.311151974492094, w0=72.62646357110825, w1=15.985169286656658\n",
      "SubSGD iter. 338/499: loss=5.310604782158317, w0=72.62646357110825, w1=15.972957469623724\n",
      "SubSGD iter. 339/499: loss=5.310576700898015, w0=72.62646357110825, w1=15.960745717164077\n",
      "SubSGD iter. 340/499: loss=5.310649170299361, w0=72.6790442559627, w1=15.994648141341948\n",
      "SubSGD iter. 341/499: loss=5.311131719267261, w0=72.62645879339568, w1=15.978516718470011\n",
      "SubSGD iter. 342/499: loss=5.310589484475882, w0=72.62645879339568, w1=15.966304936614332\n",
      "SubSGD iter. 343/499: loss=5.3106137283421795, w0=72.67903912733966, w1=16.00020713453582\n",
      "SubSGD iter. 344/499: loss=5.311148552737727, w0=72.62645349810464, w1=15.98407566053583\n",
      "SubSGD iter. 345/499: loss=5.310602267348302, w0=72.62645349810464, w1=15.97186384928574\n",
      "SubSGD iter. 346/499: loss=5.310578293467968, w0=72.67903348120828, w1=16.005765820996064\n",
      "SubSGD iter. 347/499: loss=5.311165380152711, w0=72.62644768536518, w1=15.989634295886411\n",
      "SubSGD iter. 348/499: loss=5.310615049515426, w0=72.62644768536518, w1=15.97742245524353\n",
      "SubSGD iter. 349/499: loss=5.310586968200835, w0=72.62644768536518, w1=15.965210679174064\n",
      "SubSGD iter. 350/499: loss=5.310620823905554, w0=72.67902808956227, w1=15.999112922392612\n",
      "SubSGD iter. 351/499: loss=5.311145120018227, w0=72.62644249431456, w1=15.98298145881877\n",
      "SubSGD iter. 352/499: loss=5.310599751214694, w0=72.62644249431456, w1=15.970769653354568\n",
      "SubSGD iter. 353/499: loss=5.310585387606593, w0=72.6790225476572, w1=16.004671670352856\n",
      "SubSGD iter. 354/499: loss=5.3111619486519475, w0=72.62643678578935, w1=15.98854015566565\n",
      "SubSGD iter. 355/499: loss=5.310612533523231, w0=72.62643678578935, w1=15.976328320808333\n",
      "SubSGD iter. 356/499: loss=5.310584452221943, w0=72.62643678578935, w1=15.964116550524398\n",
      "SubSGD iter. 357/499: loss=5.31062791658284, w0=72.67901726021097, w1=15.998018839021585\n",
      "SubSGD iter. 358/499: loss=5.311141689753469, w0=72.62643169892628, w1=15.981887385866436\n",
      "SubSGD iter. 359/499: loss=5.310597235377183, w0=72.62643169892628, w1=15.96967558618744\n",
      "SubSGD iter. 360/499: loss=5.310592478859825, w0=72.67901182247935, w1=16.003577648455273\n",
      "SubSGD iter. 361/499: loss=5.311158519605321, w0=72.62642609456248, w1=15.98744614418306\n",
      "SubSGD iter. 362/499: loss=5.310610017827071, w0=72.62642609456248, w1=15.975234315110626\n",
      "SubSGD iter. 363/499: loss=5.3105819365390845, w0=72.62642609456248, w1=15.963022550611543\n",
      "SubSGD iter. 364/499: loss=5.310635006375428, w0=72.67900663918006, w1=15.99692488436895\n",
      "SubSGD iter. 365/499: loss=5.311138261942243, w0=72.62642111183409, w1=15.98079344162504\n",
      "SubSGD iter. 366/499: loss=5.310594719835648, w0=72.62642111183409, w1=15.968581647730572\n",
      "SubSGD iter. 367/499: loss=5.310599567229051, w0=72.67900130556903, w1=16.002483755249543\n",
      "SubSGD iter. 368/499: loss=5.311155093011625, w0=72.62641561157882, w1=15.986352261384873\n",
      "SubSGD iter. 369/499: loss=5.310607502426823, w0=72.62641561157882, w1=15.974140438096638\n",
      "SubSGD iter. 370/499: loss=5.310579421152137, w0=72.62641561157882, w1=15.961928679381726\n",
      "SubSGD iter. 371/499: loss=5.310642093284708, w0=72.67899622636382, w1=15.995831058380947\n",
      "SubSGD iter. 372/499: loss=5.3111348365833395, w0=72.6264107329323, w1=15.979699626040832\n",
      "SubSGD iter. 373/499: loss=5.310592204589957, w0=72.6264107329323, w1=15.967487837930207\n",
      "SubSGD iter. 374/499: loss=5.31060665271566, w0=72.67899099682057, w1=16.001389990681908\n",
      "SubSGD iter. 375/499: loss=5.3111516688696465, w0=72.62640533673276, w1=15.985258507217335\n",
      "SubSGD iter. 376/499: loss=5.310604987322362, w0=72.62640533673276, w1=15.973046689712621\n",
      "SubSGD iter. 377/499: loss=5.310576906060976, w0=72.62640533673276, w1=15.960834936781199\n",
      "SubSGD iter. 378/499: loss=5.310649177312069, w0=72.67898602165664, w1=15.994737361003837\n",
      "SubSGD iter. 379/499: loss=5.311131413675551, w0=72.6264005621153, w1=15.97860593906007\n",
      "SubSGD iter. 380/499: loss=5.310589689639994, w0=72.6264005621153, w1=15.966394156732612\n",
      "SubSGD iter. 381/499: loss=5.310613735321044, w0=72.67898089612838, w1=16.00029635469865\n",
      "SubSGD iter. 382/499: loss=5.3111482471781795, w0=72.62639526991869, w1=15.984164881626732\n",
      "SubSGD iter. 383/499: loss=5.310602472513566, w0=72.62639526991869, w1=15.971953069904862\n",
      "SubSGD iter. 384/499: loss=5.310578300412999, w0=72.6789752530911, w1=16.005855041659522\n",
      "SubSGD iter. 385/499: loss=5.311165074625318, w0=72.62638946027302, w1=15.989723517477843\n",
      "SubSGD iter. 386/499: loss=5.310615254681841, w0=72.62638946027302, w1=15.97751167636318\n",
      "SubSGD iter. 387/499: loss=5.310587173366165, w0=72.62638946027302, w1=15.965299899821932\n",
      "SubSGD iter. 388/499: loss=5.310620830819769, w0=72.67896986453856, w1=15.999202143084622\n",
      "SubSGD iter. 389/499: loss=5.31114481452155, w0=72.62638427231558, w1=15.98307068043866\n",
      "SubSGD iter. 390/499: loss=5.310599956381177, w0=72.62638427231558, w1=15.970858874502673\n",
      "SubSGD iter. 391/499: loss=5.31058539448699, w0=72.67896432572634, w1=16.004760891544887\n",
      "SubSGD iter. 392/499: loss=5.31116164318741, w0=72.6263785668829, w1=15.988629377785465\n",
      "SubSGD iter. 393/499: loss=5.31061273869086, w0=72.6263785668829, w1=15.976417542456362\n",
      "SubSGD iter. 394/499: loss=5.310584657388488, w0=72.6263785668829, w1=15.964205771700644\n",
      "SubSGD iter. 395/499: loss=5.310627923432437, w0=72.67895904137234, w1=15.998108060241558\n",
      "SubSGD iter. 396/499: loss=5.311141384319634, w0=72.62637348311175, w1=15.981976608014099\n",
      "SubSGD iter. 397/499: loss=5.31059744054488, w0=72.62637348311175, w1=15.969764807863317\n",
      "SubSGD iter. 398/499: loss=5.3105924856756195, w0=72.67895360673229, w1=16.003666870174662\n",
      "SubSGD iter. 399/499: loss=5.311158214203611, w0=72.62636788183919, w1=15.987535366830041\n",
      "SubSGD iter. 400/499: loss=5.310610222995915, w0=72.62636788183919, w1=15.975323537285817\n",
      "SubSGD iter. 401/499: loss=5.310582141706844, w0=72.62636788183919, w1=15.963111772314948\n",
      "SubSGD iter. 402/499: loss=5.31063501316044, w0=72.67894842652395, w1=15.99701410611567\n",
      "SubSGD iter. 403/499: loss=5.311137956571222, w0=72.62636290220146, w1=15.98088266429926\n",
      "SubSGD iter. 404/499: loss=5.310594925004553, w0=72.62636290220146, w1=15.968670869933002\n",
      "SubSGD iter. 405/499: loss=5.310599573980277, w0=72.67894309600325, w1=16.00257297749507\n",
      "SubSGD iter. 406/499: loss=5.311154787672715, w0=72.62635740503619, w1=15.986441484557803\n",
      "SubSGD iter. 407/499: loss=5.3106077075968745, w0=72.62635740503619, w1=15.974229660797777\n",
      "SubSGD iter. 408/499: loss=5.310579626321105, w0=72.62635740503619, w1=15.962017901611075\n",
      "SubSGD iter. 409/499: loss=5.310642100005169, w0=72.67893801988772, w1=15.995920280653198\n",
      "SubSGD iter. 410/499: loss=5.311134531275101, w0=72.62635252947906, w1=15.979788849240391\n",
      "SubSGD iter. 411/499: loss=5.310592409760071, w0=72.62635252947906, w1=15.967577060657975\n",
      "SubSGD iter. 412/499: loss=5.310606659402351, w0=72.67893279343355, w1=16.001479213452363\n",
      "SubSGD iter. 413/499: loss=5.311151363593508, w0=72.62634713636827, w1=15.985347730915\n",
      "SubSGD iter. 414/499: loss=5.310605192493623, w0=72.62634713636827, w1=15.973135912938494\n",
      "SubSGD iter. 415/499: loss=5.310577111231151, w0=72.62634713636827, w1=15.96092415953528\n",
      "SubSGD iter. 416/499: loss=5.31064918396801, w0=72.67892782135804, w1=15.994826583800409\n",
      "SubSGD iter. 417/499: loss=5.311131108430068, w0=72.62634236483893, w1=15.978695162783758\n",
      "SubSGD iter. 418/499: loss=5.310589894811314, w0=72.62634236483893, w1=15.966483379984506\n",
      "SubSGD iter. 419/499: loss=5.31061374194323, w0=72.67892269891757, w1=16.000385577992823\n",
      "SubSGD iter. 420/499: loss=5.31114794196478, w0=72.62633707572981, w1=15.984254105847924\n",
      "SubSGD iter. 421/499: loss=5.310602677686031, w0=72.62633707572981, w1=15.972042293654255\n",
      "SubSGD iter. 422/499: loss=5.310578307001441, w0=72.67891705896744, w1=16.005944265450974\n",
      "SubSGD iter. 423/499: loss=5.311164769443996, w0=72.62633126917096, w1=15.989812742196218\n",
      "SubSGD iter. 424/499: loss=5.310615459855448, w0=72.62633126917096, w1=15.977600900609755\n",
      "SubSGD iter. 425/499: loss=5.310587378538686, w0=72.62633126917096, w1=15.96538912359671\n",
      "SubSGD iter. 426/499: loss=5.310620837377486, w0=72.67891167350143, w1=15.999291366901263\n",
      "SubSGD iter. 427/499: loss=5.311144509370863, w0=72.62632608429973, w1=15.98315990518213\n",
      "SubSGD iter. 428/499: loss=5.310600161554841, w0=72.62632608429973, w1=15.970948098774345\n",
      "SubSGD iter. 429/499: loss=5.3105854010109805, w0=72.67890613777509, w1=16.004850115858204\n",
      "SubSGD iter. 430/499: loss=5.311161338068787, w0=72.62632038195262, w1=15.988718603025514\n",
      "SubSGD iter. 431/499: loss=5.310612943865668, w0=72.62632038195262, w1=15.976506767224608\n",
      "SubSGD iter. 432/499: loss=5.3105848625622105, w0=72.62632038195262, w1=15.96429499599709\n",
      "SubSGD iter. 433/499: loss=5.310627929925717, w0=72.67890085650635, w1=15.998197284579454\n",
      "SubSGD iter. 434/499: loss=5.311141079231631, w0=72.62631530126643, w1=15.982065833278634\n",
      "SubSGD iter. 435/499: loss=5.310597645719742, w0=72.62631530126643, w1=15.969854032656048\n",
      "SubSGD iter. 436/499: loss=5.31059249213519, w0=72.67889542495094, w1=16.00375609500863\n",
      "SubSGD iter. 437/499: loss=5.311157909147658, w0=72.62630970307819, w1=15.987624592590551\n",
      "SubSGD iter. 438/499: loss=5.310610428171917, w0=72.62630970307819, w1=15.975412762574521\n",
      "SubSGD iter. 439/499: loss=5.31058234688176, w0=72.62630970307819, w1=15.963200997131848\n",
      "SubSGD iter. 440/499: loss=5.310635019589317, w0=72.6788902478266, w1=15.99710333097361\n",
      "SubSGD iter. 441/499: loss=5.311137651545873, w0=72.62630472652417, w1=15.98097189008365\n",
      "SubSGD iter. 442/499: loss=5.3105951301806105, w0=72.62630472652417, w1=15.968760095245585\n",
      "SubSGD iter. 443/499: loss=5.310599580375458, w0=72.67888492038927, w1=16.00266220284848\n",
      "SubSGD iter. 444/499: loss=5.3111544826794, w0=72.62629923244195, w1=15.986530710837563\n",
      "SubSGD iter. 445/499: loss=5.3106079127740715, w0=72.62629923244195, w1=15.97431888660573\n",
      "SubSGD iter. 446/499: loss=5.310579831497216, w0=72.62629923244195, w1=15.962107126947222\n",
      "SubSGD iter. 447/499: loss=5.3106421063696745, w0=72.6788798473565, w1=15.996009506029976\n",
      "SubSGD iter. 448/499: loss=5.31113422631238, w0=72.62629435996726, w1=15.979878075543429\n",
      "SubSGD iter. 449/499: loss=5.310592614937323, w0=72.62629435996726, w1=15.967666286489205\n",
      "SubSGD iter. 450/499: loss=5.310606665733177, w0=72.67887462398443, w1=16.001568439324007\n",
      "SubSGD iter. 451/499: loss=5.311151058662806, w0=72.62628896993826, w1=15.985436957712807\n",
      "SubSGD iter. 452/499: loss=5.310605397672011, w0=72.62628896993826, w1=15.973225139264489\n",
      "SubSGD iter. 453/499: loss=5.3105773164084535, w0=72.62628896993826, w1=15.961013385389467\n",
      "SubSGD iter. 454/499: loss=5.310649190268176, w0=72.67886965499042, w1=15.994915809694815\n",
      "SubSGD iter. 455/499: loss=5.311130803529947, w0=72.62628420149012, w1=15.978784389604234\n",
      "SubSGD iter. 456/499: loss=5.310590099989755, w0=72.62628420149012, w1=15.966572606333171\n",
      "SubSGD iter. 457/499: loss=5.310613748209733, w0=72.67886453563081, w1=16.00047480438149\n",
      "SubSGD iter. 458/499: loss=5.311147637096662, w0=72.62627891546154, w1=15.984343333162562\n",
      "SubSGD iter. 459/499: loss=5.310602882865607, w0=72.62627891546154, w1=15.97213152049708\n",
      "SubSGD iter. 460/499: loss=5.310578313234288, w0=72.6788588987609, w1=16.00603349233359\n",
      "SubSGD iter. 461/499: loss=5.311164464607875, w0=72.6262731119826, w1=15.989901970004707\n",
      "SubSGD iter. 462/499: loss=5.310615665036161, w0=72.6262731119826, w1=15.977690127946428\n",
      "SubSGD iter. 463/499: loss=5.310587583718312, w0=72.6262731119826, w1=15.965478350461568\n",
      "SubSGD iter. 464/499: loss=5.310620843579698, w0=72.67885351637447, w1=15.999380593805714\n",
      "SubSGD iter. 465/499: loss=5.3111442045652995, w0=72.62626793019066, w1=15.983249133012364\n",
      "SubSGD iter. 466/499: loss=5.3106003667356045, w0=72.62626793019066, w1=15.971037326132763\n",
      "SubSGD iter. 467/499: loss=5.310585407179554, w0=72.6788479837271, w1=16.004939343256\n",
      "SubSGD iter. 468/499: loss=5.311161033295207, w0=72.6262622309222, w1=15.988807831348996\n",
      "SubSGD iter. 469/499: loss=5.310613149047564, w0=72.6262622309222, w1=15.976595995076272\n",
      "SubSGD iter. 470/499: loss=5.31058506774302, w0=72.6262622309222, w1=15.964384223376937\n",
      "SubSGD iter. 471/499: loss=5.310627936063674, w0=72.67884270553668, w1=15.998286511998487\n",
      "SubSGD iter. 472/499: loss=5.311140774488593, w0=72.62625715331401, w1=15.982155061623258\n",
      "SubSGD iter. 473/499: loss=5.310597850901687, w0=72.62625715331401, w1=15.969943260528854\n",
      "SubSGD iter. 474/499: loss=5.310592498239522, w0=72.67883727705896, w1=16.003845322920405\n",
      "SubSGD iter. 475/499: loss=5.311157604436589, w0=72.62625155820315, w1=15.987713821427821\n",
      "SubSGD iter. 476/499: loss=5.310610633354995, w0=72.62625155820315, w1=15.97550199093997\n",
      "SubSGD iter. 477/499: loss=5.310582552063754, w0=72.62625155820315, w1=15.963290225025478\n",
      "SubSGD iter. 478/499: loss=5.310635025663048, w0=72.6788321030117, w1=15.997192558906015\n",
      "SubSGD iter. 479/499: loss=5.311137346865334, w0=72.6262465847259, w1=15.981061118941456\n",
      "SubSGD iter. 480/499: loss=5.310595335363735, w0=72.6262465847259, w1=15.96884932363157\n",
      "SubSGD iter. 481/499: loss=5.310599586415582, w0=72.67882677865082, w1=16.002751431273023\n",
      "SubSGD iter. 482/499: loss=5.311154178030816, w0=72.62624109371981, w1=15.986619940187412\n",
      "SubSGD iter. 483/499: loss=5.310608117958329, w0=72.62624109371981, w1=15.974408115483753\n",
      "SubSGD iter. 484/499: loss=5.310580036680389, w0=72.62624109371981, w1=15.962196355353424\n",
      "SubSGD iter. 485/499: loss=5.310642112379212, w0=72.67882170869386, w1=15.996098734474542\n",
      "SubSGD iter. 486/499: loss=5.311133921694311, w0=72.62623622432065, w1=15.979967304913206\n",
      "SubSGD iter. 487/499: loss=5.310592820121625, w0=72.62623622432065, w1=15.967755515387157\n",
      "SubSGD iter. 488/499: loss=5.310606671709126, w0=72.67881648839698, w1=16.001657668260112\n",
      "SubSGD iter. 489/499: loss=5.311150754076675, w0=72.6262308373665, w1=15.985526187574026\n",
      "SubSGD iter. 490/499: loss=5.310605602857443, w0=72.6262308373665, w1=15.973314368653881\n",
      "SubSGD iter. 491/499: loss=5.310577521592802, w0=72.6262308373665, w1=15.961102614307034\n",
      "SubSGD iter. 492/499: loss=5.310649196213555, w0=72.67881152247753, w1=15.995005038650337\n",
      "SubSGD iter. 493/499: loss=5.311130498974316, w0=72.62622607199263, w1=15.978873619484778\n",
      "SubSGD iter. 494/499: loss=5.310590305175231, w0=72.62622607199263, w1=15.966661835741888\n",
      "SubSGD iter. 495/499: loss=5.310613754121539, w0=72.67880640619185, w1=16.00056403382795\n",
      "SubSGD iter. 496/499: loss=5.311147332572959, w0=72.62622078903766, w1=15.984432563533947\n",
      "SubSGD iter. 497/499: loss=5.310603088052212, w0=72.62622078903766, w1=15.972220750396634\n",
      "SubSGD iter. 498/499: loss=5.310578319112527, w0=72.6788007723952, w1=16.006122722270668\n",
      "SubSGD iter. 499/499: loss=5.311164160116089, w0=72.62621498863169, w1=15.989991200866614\n",
      "SubSGD: execution time=0.037 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ce1d51f7ca4adc85d47d6a83e1df7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=1, min=1), Output()), _dom_classes=('widget…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses, subsgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
